{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2b8da2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-16T13:23:21.330537Z",
     "iopub.status.busy": "2025-04-16T13:23:21.330184Z",
     "iopub.status.idle": "2025-04-16T13:23:23.460145Z",
     "shell.execute_reply": "2025-04-16T13:23:23.458913Z"
    },
    "papermill": {
     "duration": 2.136761,
     "end_time": "2025-04-16T13:23:23.462011",
     "exception": false,
     "start_time": "2025-04-16T13:23:21.325250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/article-classification/train.csv\n",
      "/kaggle/input/article-classification/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822f7b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T13:23:23.469773Z",
     "iopub.status.busy": "2025-04-16T13:23:23.469042Z",
     "iopub.status.idle": "2025-04-16T23:28:15.172675Z",
     "shell.execute_reply": "2025-04-16T23:28:15.171435Z"
    },
    "papermill": {
     "duration": 36291.709482,
     "end_time": "2025-04-16T23:28:15.174135",
     "exception": false,
     "start_time": "2025-04-16T13:23:23.464653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preprocessing ---\n",
      "Error: train.csv or test.csv not found.\n",
      "Loaded data from Kaggle path.\n",
      "Loaded train.csv: (7769, 2)\n",
      "Loaded test.csv: (3019, 2)\n",
      "Handled potential missing values.\n",
      "Applying text cleaning to documents...\n",
      "Text cleaning complete.\n",
      "\n",
      "Parsing and binarizing labels...\n",
      "Number of unique labels found: 90\n",
      "Shape of binarized training labels: (7769, 90)\n",
      "Shape of binarized test labels: (3019, 90)\n",
      "Labels binarized.\n",
      "\n",
      "Calculating TF-IDF features from scratch...\n",
      "Vocabulary size limited to top 5000 features by IDF.\n",
      "Shape of TF-IDF training features: (7769, 5000)\n",
      "Shape of TF-IDF test features: (3019, 5000)\n",
      "TF-IDF calculation complete.\n",
      "\n",
      "Split training data into training and validation sets:\n",
      "X_train shape: (6603, 5000), y_train shape: (6603, 90)\n",
      "X_val shape: (1166, 5000), y_val shape: (1166, 90)\n",
      "X_test shape: (3019, 5000), y_test shape: (3019, 90)\n",
      "\n",
      "MLPClassifierMultiLabel class defined (with Momentum).\n",
      "\n",
      "--- Hyperparameter Tuning & Evaluation ---\n",
      "Input features: 5000, Output labels: 90\n",
      "Data shapes for MLP: \n",
      "  X_train_mlp: (5000, 6603), y_train_mlp: (90, 6603)\n",
      "  X_val_mlp:   (5000, 1166), y_val_mlp:   (90, 1166)\n",
      "  X_test_mlp:  (5000, 3019), y_test_bin:  (3019, 90)\n",
      "\n",
      "--- Running Iteration 1/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 23.937121 | Val Cost: 5.069299 | Time: 2.92s\n",
      "Epoch 20/149 | Train Cost: 4.395617 | Val Cost: 4.506236 | Time: 2.73s\n",
      "Epoch 40/149 | Train Cost: 4.394793 | Val Cost: 4.503112 | Time: 2.89s\n",
      "Epoch 60/149 | Train Cost: 4.406151 | Val Cost: 4.507165 | Time: 2.92s\n",
      "Epoch 80/149 | Train Cost: 4.392126 | Val Cost: 4.504748 | Time: 3.36s\n",
      "Epoch 100/149 | Train Cost: 4.395521 | Val Cost: 4.503892 | Time: 3.47s\n",
      "Epoch 120/149 | Train Cost: 4.395723 | Val Cost: 4.502602 | Time: 3.55s\n",
      "Epoch 140/149 | Train Cost: 4.391048 | Val Cost: 4.499659 | Time: 3.03s\n",
      "Epoch 149/149 | Train Cost: 4.392466 | Val Cost: 4.501246 | Time: 3.01s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 483.72 seconds\n",
      "\n",
      "--- Running Iteration 2/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 41.400122 | Val Cost: 10.785017 | Time: 2.10s\n",
      "Epoch 20/149 | Train Cost: 4.395137 | Val Cost: 4.506882 | Time: 1.88s\n",
      "Epoch 40/149 | Train Cost: 4.390349 | Val Cost: 4.504431 | Time: 1.97s\n",
      "Epoch 60/149 | Train Cost: 4.418147 | Val Cost: 4.502845 | Time: 2.06s\n",
      "Epoch 80/149 | Train Cost: 4.385275 | Val Cost: 4.502146 | Time: 2.26s\n",
      "Epoch 100/149 | Train Cost: 4.392280 | Val Cost: 4.503160 | Time: 2.00s\n",
      "Epoch 120/149 | Train Cost: 4.393862 | Val Cost: 4.501857 | Time: 2.62s\n",
      "Epoch 140/149 | Train Cost: 4.384910 | Val Cost: 4.501168 | Time: 2.22s\n",
      "Epoch 149/149 | Train Cost: 4.388017 | Val Cost: 4.501209 | Time: 2.43s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 311.29 seconds\n",
      "\n",
      "--- Running Iteration 3/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 22.226269 | Val Cost: 5.381011 | Time: 3.38s\n",
      "Epoch 20/149 | Train Cost: 4.396769 | Val Cost: 4.506557 | Time: 3.31s\n",
      "Epoch 40/149 | Train Cost: 4.394615 | Val Cost: 4.504113 | Time: 3.20s\n",
      "Epoch 60/149 | Train Cost: 4.405675 | Val Cost: 4.504780 | Time: 3.16s\n",
      "Epoch 80/149 | Train Cost: 4.391772 | Val Cost: 4.503802 | Time: 3.18s\n",
      "Epoch 100/149 | Train Cost: 4.394860 | Val Cost: 4.503687 | Time: 3.19s\n",
      "Epoch 120/149 | Train Cost: 4.395563 | Val Cost: 4.502852 | Time: 3.19s\n",
      "Epoch 140/149 | Train Cost: 4.391210 | Val Cost: 4.500614 | Time: 3.18s\n",
      "Epoch 149/149 | Train Cost: 4.392615 | Val Cost: 4.501618 | Time: 2.73s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 454.54 seconds\n",
      "\n",
      "--- Running Iteration 4/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 37.732051 | Val Cost: 9.981939 | Time: 2.07s\n",
      "Epoch 20/149 | Train Cost: 4.401509 | Val Cost: 4.512660 | Time: 2.17s\n",
      "Epoch 40/149 | Train Cost: 4.391940 | Val Cost: 4.505991 | Time: 2.14s\n",
      "Epoch 60/149 | Train Cost: 4.418702 | Val Cost: 4.503817 | Time: 2.19s\n",
      "Epoch 80/149 | Train Cost: 4.385657 | Val Cost: 4.502667 | Time: 1.93s\n",
      "Epoch 100/149 | Train Cost: 4.392667 | Val Cost: 4.504118 | Time: 1.93s\n",
      "Epoch 120/149 | Train Cost: 4.393839 | Val Cost: 4.502542 | Time: 2.85s\n",
      "Epoch 140/149 | Train Cost: 4.385226 | Val Cost: 4.502613 | Time: 1.98s\n",
      "Epoch 149/149 | Train Cost: 4.388274 | Val Cost: 4.501861 | Time: 2.12s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 314.14 seconds\n",
      "\n",
      "--- Running Iteration 5/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 16.589156 | Val Cost: 4.542642 | Time: 6.20s\n",
      "Epoch 20/149 | Train Cost: 4.403521 | Val Cost: 4.512641 | Time: 5.69s\n",
      "Epoch 40/149 | Train Cost: 4.401487 | Val Cost: 4.508152 | Time: 6.29s\n",
      "Epoch 60/149 | Train Cost: 4.414656 | Val Cost: 4.516852 | Time: 6.34s\n",
      "Epoch 80/149 | Train Cost: 4.399728 | Val Cost: 4.507873 | Time: 6.43s\n",
      "Epoch 100/149 | Train Cost: 4.401978 | Val Cost: 4.511900 | Time: 6.76s\n",
      "Epoch 120/149 | Train Cost: 4.399474 | Val Cost: 4.504613 | Time: 6.40s\n",
      "Epoch 140/149 | Train Cost: 4.391628 | Val Cost: 4.499637 | Time: 6.45s\n",
      "Epoch 149/149 | Train Cost: 4.392934 | Val Cost: 4.505915 | Time: 6.58s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 951.58 seconds\n",
      "\n",
      "--- Running Iteration 6/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 28.522715 | Val Cost: 4.649272 | Time: 3.52s\n",
      "Epoch 20/149 | Train Cost: 4.393887 | Val Cost: 4.507863 | Time: 3.49s\n",
      "Epoch 40/149 | Train Cost: 4.394747 | Val Cost: 4.505007 | Time: 4.17s\n",
      "Epoch 60/149 | Train Cost: 4.422767 | Val Cost: 4.512336 | Time: 3.52s\n",
      "Epoch 80/149 | Train Cost: 4.389203 | Val Cost: 4.509038 | Time: 3.91s\n",
      "Epoch 100/149 | Train Cost: 4.397626 | Val Cost: 4.504838 | Time: 3.92s\n",
      "Epoch 120/149 | Train Cost: 4.398281 | Val Cost: 4.503539 | Time: 3.94s\n",
      "Epoch 140/149 | Train Cost: 4.388304 | Val Cost: 4.498624 | Time: 4.09s\n",
      "Epoch 149/149 | Train Cost: 4.392271 | Val Cost: 4.501200 | Time: 3.96s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 575.96 seconds\n",
      "\n",
      "--- Running Iteration 7/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 15.302278 | Val Cost: 4.677275 | Time: 5.86s\n",
      "Epoch 20/149 | Train Cost: 4.398134 | Val Cost: 4.509368 | Time: 5.87s\n",
      "Epoch 40/149 | Train Cost: 4.397834 | Val Cost: 4.504649 | Time: 5.88s\n",
      "Epoch 60/149 | Train Cost: 4.410284 | Val Cost: 4.515439 | Time: 5.67s\n",
      "Epoch 80/149 | Train Cost: 4.396862 | Val Cost: 4.507569 | Time: 5.87s\n",
      "Epoch 100/149 | Train Cost: 4.401226 | Val Cost: 4.506935 | Time: 5.75s\n",
      "Epoch 120/149 | Train Cost: 4.400795 | Val Cost: 4.503286 | Time: 5.65s\n",
      "Epoch 140/149 | Train Cost: 4.396002 | Val Cost: 4.500638 | Time: 6.27s\n",
      "Epoch 149/149 | Train Cost: 4.397891 | Val Cost: 4.503210 | Time: 5.88s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 877.44 seconds\n",
      "\n",
      "--- Running Iteration 8/48 ---\n",
      "Config: LR=0.01, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 25.781201 | Val Cost: 5.098151 | Time: 3.52s\n",
      "Epoch 20/149 | Train Cost: 4.393562 | Val Cost: 4.505318 | Time: 3.69s\n",
      "Epoch 40/149 | Train Cost: 4.391850 | Val Cost: 4.505561 | Time: 3.67s\n",
      "Epoch 60/149 | Train Cost: 4.420372 | Val Cost: 4.503515 | Time: 3.60s\n",
      "Epoch 80/149 | Train Cost: 4.387379 | Val Cost: 4.503741 | Time: 3.63s\n",
      "Epoch 100/149 | Train Cost: 4.394752 | Val Cost: 4.503580 | Time: 3.69s\n",
      "Epoch 120/149 | Train Cost: 4.396884 | Val Cost: 4.502663 | Time: 3.57s\n",
      "Epoch 140/149 | Train Cost: 4.387555 | Val Cost: 4.500359 | Time: 4.37s\n",
      "Epoch 149/149 | Train Cost: 4.391046 | Val Cost: 4.501696 | Time: 3.74s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 559.80 seconds\n",
      "\n",
      "--- Running Iteration 9/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 23.937121 | Val Cost: 5.069299 | Time: 2.63s\n",
      "Epoch 20/249 | Train Cost: 4.395617 | Val Cost: 4.506236 | Time: 2.63s\n",
      "Epoch 40/249 | Train Cost: 4.394793 | Val Cost: 4.503112 | Time: 2.84s\n",
      "Epoch 60/249 | Train Cost: 4.406151 | Val Cost: 4.507165 | Time: 3.05s\n",
      "Epoch 80/249 | Train Cost: 4.392126 | Val Cost: 4.504748 | Time: 3.00s\n",
      "Epoch 100/249 | Train Cost: 4.395521 | Val Cost: 4.503892 | Time: 2.95s\n",
      "Epoch 120/249 | Train Cost: 4.395723 | Val Cost: 4.502602 | Time: 3.03s\n",
      "Epoch 140/249 | Train Cost: 4.391048 | Val Cost: 4.499659 | Time: 3.19s\n",
      "Epoch 160/249 | Train Cost: 4.392407 | Val Cost: 4.507020 | Time: 2.97s\n",
      "Epoch 180/249 | Train Cost: 4.392517 | Val Cost: 4.507341 | Time: 3.11s\n",
      "Epoch 200/249 | Train Cost: 4.394254 | Val Cost: 4.505131 | Time: 2.99s\n",
      "Epoch 220/249 | Train Cost: 4.387450 | Val Cost: 4.509996 | Time: 2.96s\n",
      "Epoch 240/249 | Train Cost: 4.397259 | Val Cost: 4.502428 | Time: 3.02s\n",
      "Epoch 249/249 | Train Cost: 4.386419 | Val Cost: 4.501683 | Time: 2.94s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 744.29 seconds\n",
      "\n",
      "--- Running Iteration 10/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 41.400122 | Val Cost: 10.785017 | Time: 1.84s\n",
      "Epoch 20/249 | Train Cost: 4.395137 | Val Cost: 4.506882 | Time: 1.75s\n",
      "Epoch 40/249 | Train Cost: 4.390349 | Val Cost: 4.504431 | Time: 1.77s\n",
      "Epoch 60/249 | Train Cost: 4.418147 | Val Cost: 4.502845 | Time: 1.91s\n",
      "Epoch 80/249 | Train Cost: 4.385275 | Val Cost: 4.502146 | Time: 2.60s\n",
      "Epoch 100/249 | Train Cost: 4.392280 | Val Cost: 4.503160 | Time: 2.09s\n",
      "Epoch 120/249 | Train Cost: 4.393862 | Val Cost: 4.501857 | Time: 2.02s\n",
      "Epoch 140/249 | Train Cost: 4.384910 | Val Cost: 4.501168 | Time: 2.02s\n",
      "Epoch 160/249 | Train Cost: 4.391027 | Val Cost: 4.502243 | Time: 2.05s\n",
      "Epoch 180/249 | Train Cost: 4.390850 | Val Cost: 4.505070 | Time: 2.20s\n",
      "Epoch 200/249 | Train Cost: 4.395354 | Val Cost: 4.502465 | Time: 2.05s\n",
      "Epoch 220/249 | Train Cost: 4.379481 | Val Cost: 4.502360 | Time: 2.04s\n",
      "Epoch 240/249 | Train Cost: 4.404686 | Val Cost: 4.501144 | Time: 1.98s\n",
      "Epoch 249/249 | Train Cost: 4.380856 | Val Cost: 4.501099 | Time: 2.52s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 505.01 seconds\n",
      "\n",
      "--- Running Iteration 11/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 22.226269 | Val Cost: 5.381011 | Time: 2.61s\n",
      "Epoch 20/249 | Train Cost: 4.396769 | Val Cost: 4.506557 | Time: 2.76s\n",
      "Epoch 40/249 | Train Cost: 4.394615 | Val Cost: 4.504113 | Time: 2.69s\n",
      "Epoch 60/249 | Train Cost: 4.405675 | Val Cost: 4.504780 | Time: 2.75s\n",
      "Epoch 80/249 | Train Cost: 4.391772 | Val Cost: 4.503802 | Time: 2.72s\n",
      "Epoch 100/249 | Train Cost: 4.394860 | Val Cost: 4.503687 | Time: 2.67s\n",
      "Epoch 120/249 | Train Cost: 4.395563 | Val Cost: 4.502852 | Time: 2.62s\n",
      "Epoch 140/249 | Train Cost: 4.391210 | Val Cost: 4.500614 | Time: 2.65s\n",
      "Epoch 160/249 | Train Cost: 4.392998 | Val Cost: 4.505391 | Time: 2.76s\n",
      "Epoch 180/249 | Train Cost: 4.393205 | Val Cost: 4.506667 | Time: 3.09s\n",
      "Epoch 200/249 | Train Cost: 4.395161 | Val Cost: 4.504835 | Time: 2.96s\n",
      "Epoch 220/249 | Train Cost: 4.388365 | Val Cost: 4.507876 | Time: 3.37s\n",
      "Epoch 240/249 | Train Cost: 4.398472 | Val Cost: 4.502462 | Time: 3.03s\n",
      "Epoch 249/249 | Train Cost: 4.388177 | Val Cost: 4.502513 | Time: 2.78s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 700.03 seconds\n",
      "\n",
      "--- Running Iteration 12/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 37.732051 | Val Cost: 9.981939 | Time: 1.87s\n",
      "Epoch 20/249 | Train Cost: 4.401509 | Val Cost: 4.512660 | Time: 1.89s\n",
      "Epoch 40/249 | Train Cost: 4.391940 | Val Cost: 4.505991 | Time: 1.81s\n",
      "Epoch 60/249 | Train Cost: 4.418702 | Val Cost: 4.503817 | Time: 1.91s\n",
      "Epoch 80/249 | Train Cost: 4.385657 | Val Cost: 4.502667 | Time: 2.04s\n",
      "Epoch 100/249 | Train Cost: 4.392667 | Val Cost: 4.504118 | Time: 2.40s\n",
      "Epoch 120/249 | Train Cost: 4.393839 | Val Cost: 4.502542 | Time: 1.96s\n",
      "Epoch 140/249 | Train Cost: 4.385226 | Val Cost: 4.502613 | Time: 1.94s\n",
      "Epoch 160/249 | Train Cost: 4.391360 | Val Cost: 4.502151 | Time: 1.98s\n",
      "Epoch 180/249 | Train Cost: 4.391143 | Val Cost: 4.503906 | Time: 2.16s\n",
      "Epoch 200/249 | Train Cost: 4.395464 | Val Cost: 4.502324 | Time: 2.16s\n",
      "Epoch 220/249 | Train Cost: 4.380125 | Val Cost: 4.502019 | Time: 1.96s\n",
      "Epoch 240/249 | Train Cost: 4.405183 | Val Cost: 4.501717 | Time: 2.06s\n",
      "Epoch 249/249 | Train Cost: 4.381279 | Val Cost: 4.501116 | Time: 2.02s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 506.12 seconds\n",
      "\n",
      "--- Running Iteration 13/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 16.589156 | Val Cost: 4.542642 | Time: 5.70s\n",
      "Epoch 20/249 | Train Cost: 4.403521 | Val Cost: 4.512641 | Time: 5.55s\n",
      "Epoch 40/249 | Train Cost: 4.401487 | Val Cost: 4.508152 | Time: 6.64s\n",
      "Epoch 60/249 | Train Cost: 4.414656 | Val Cost: 4.516852 | Time: 6.48s\n",
      "Epoch 80/249 | Train Cost: 4.399728 | Val Cost: 4.507873 | Time: 6.33s\n",
      "Epoch 100/249 | Train Cost: 4.401978 | Val Cost: 4.511900 | Time: 6.36s\n",
      "Epoch 120/249 | Train Cost: 4.399474 | Val Cost: 4.504613 | Time: 6.63s\n",
      "Epoch 140/249 | Train Cost: 4.391628 | Val Cost: 4.499637 | Time: 6.44s\n",
      "Epoch 160/249 | Train Cost: 4.390386 | Val Cost: 4.517650 | Time: 6.67s\n",
      "Epoch 180/249 | Train Cost: 4.387685 | Val Cost: 4.510611 | Time: 6.73s\n",
      "Epoch 200/249 | Train Cost: 4.384639 | Val Cost: 4.504117 | Time: 6.99s\n",
      "Epoch 220/249 | Train Cost: 4.371131 | Val Cost: 4.506081 | Time: 6.95s\n",
      "Epoch 240/249 | Train Cost: 4.369397 | Val Cost: 4.500138 | Time: 6.88s\n",
      "Epoch 249/249 | Train Cost: 4.350006 | Val Cost: 4.501492 | Time: 6.25s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0010\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0016\n",
      "  F1 Macro         = 0.0001\n",
      "  F1 Samples       = 0.0010\n",
      "Training Time: 1606.21 seconds\n",
      "\n",
      "--- Running Iteration 14/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 28.522715 | Val Cost: 4.649272 | Time: 3.24s\n",
      "Epoch 20/249 | Train Cost: 4.393887 | Val Cost: 4.507863 | Time: 4.27s\n",
      "Epoch 40/249 | Train Cost: 4.394747 | Val Cost: 4.505007 | Time: 3.53s\n",
      "Epoch 60/249 | Train Cost: 4.422767 | Val Cost: 4.512336 | Time: 3.54s\n",
      "Epoch 80/249 | Train Cost: 4.389203 | Val Cost: 4.509038 | Time: 3.91s\n",
      "Epoch 100/249 | Train Cost: 4.397626 | Val Cost: 4.504838 | Time: 3.67s\n",
      "Epoch 120/249 | Train Cost: 4.398281 | Val Cost: 4.503539 | Time: 3.65s\n",
      "Epoch 140/249 | Train Cost: 4.388304 | Val Cost: 4.498624 | Time: 3.96s\n",
      "Epoch 160/249 | Train Cost: 4.393649 | Val Cost: 4.511019 | Time: 3.73s\n",
      "Epoch 180/249 | Train Cost: 4.394249 | Val Cost: 4.511056 | Time: 4.03s\n",
      "Epoch 200/249 | Train Cost: 4.398147 | Val Cost: 4.508268 | Time: 3.97s\n",
      "Epoch 220/249 | Train Cost: 4.381363 | Val Cost: 4.517166 | Time: 3.98s\n",
      "Epoch 240/249 | Train Cost: 4.406402 | Val Cost: 4.504144 | Time: 4.05s\n",
      "Epoch 249/249 | Train Cost: 4.381756 | Val Cost: 4.501904 | Time: 3.74s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 968.32 seconds\n",
      "\n",
      "--- Running Iteration 15/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 15.302278 | Val Cost: 4.677275 | Time: 6.21s\n",
      "Epoch 20/249 | Train Cost: 4.398134 | Val Cost: 4.509368 | Time: 5.73s\n",
      "Epoch 40/249 | Train Cost: 4.397834 | Val Cost: 4.504649 | Time: 5.72s\n",
      "Epoch 60/249 | Train Cost: 4.410284 | Val Cost: 4.515439 | Time: 6.29s\n",
      "Epoch 80/249 | Train Cost: 4.396862 | Val Cost: 4.507569 | Time: 5.96s\n",
      "Epoch 100/249 | Train Cost: 4.401226 | Val Cost: 4.506935 | Time: 6.00s\n",
      "Epoch 120/249 | Train Cost: 4.400795 | Val Cost: 4.503286 | Time: 5.92s\n",
      "Epoch 140/249 | Train Cost: 4.396002 | Val Cost: 4.500638 | Time: 6.04s\n",
      "Epoch 160/249 | Train Cost: 4.397442 | Val Cost: 4.512767 | Time: 5.94s\n",
      "Epoch 180/249 | Train Cost: 4.398035 | Val Cost: 4.511290 | Time: 5.89s\n",
      "Epoch 200/249 | Train Cost: 4.400445 | Val Cost: 4.507075 | Time: 5.86s\n",
      "Epoch 220/249 | Train Cost: 4.394779 | Val Cost: 4.511691 | Time: 5.77s\n",
      "Epoch 240/249 | Train Cost: 4.404778 | Val Cost: 4.504566 | Time: 5.86s\n",
      "Epoch 249/249 | Train Cost: 4.393111 | Val Cost: 4.503024 | Time: 6.60s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1482.94 seconds\n",
      "\n",
      "--- Running Iteration 16/48 ---\n",
      "Config: LR=0.01, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.01, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 25.781201 | Val Cost: 5.098151 | Time: 3.57s\n",
      "Epoch 20/249 | Train Cost: 4.393562 | Val Cost: 4.505318 | Time: 3.59s\n",
      "Epoch 40/249 | Train Cost: 4.391850 | Val Cost: 4.505561 | Time: 3.40s\n",
      "Epoch 60/249 | Train Cost: 4.420372 | Val Cost: 4.503515 | Time: 3.71s\n",
      "Epoch 80/249 | Train Cost: 4.387379 | Val Cost: 4.503741 | Time: 3.68s\n",
      "Epoch 100/249 | Train Cost: 4.394752 | Val Cost: 4.503580 | Time: 3.59s\n",
      "Epoch 120/249 | Train Cost: 4.396884 | Val Cost: 4.502663 | Time: 3.78s\n",
      "Epoch 140/249 | Train Cost: 4.387555 | Val Cost: 4.500359 | Time: 3.62s\n",
      "Epoch 160/249 | Train Cost: 4.393877 | Val Cost: 4.505186 | Time: 3.68s\n",
      "Epoch 180/249 | Train Cost: 4.394411 | Val Cost: 4.508886 | Time: 3.40s\n",
      "Epoch 200/249 | Train Cost: 4.399641 | Val Cost: 4.505008 | Time: 4.00s\n",
      "Epoch 220/249 | Train Cost: 4.382801 | Val Cost: 4.507368 | Time: 3.54s\n",
      "Epoch 240/249 | Train Cost: 4.408659 | Val Cost: 4.502516 | Time: 3.66s\n",
      "Epoch 249/249 | Train Cost: 4.385031 | Val Cost: 4.503222 | Time: 3.70s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 914.75 seconds\n",
      "\n",
      "--- Running Iteration 17/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 37.822279 | Val Cost: 9.243760 | Time: 2.64s\n",
      "Epoch 20/149 | Train Cost: 4.398410 | Val Cost: 4.507694 | Time: 2.73s\n",
      "Epoch 40/149 | Train Cost: 4.393932 | Val Cost: 4.504267 | Time: 2.99s\n",
      "Epoch 60/149 | Train Cost: 4.404474 | Val Cost: 4.503476 | Time: 3.20s\n",
      "Epoch 80/149 | Train Cost: 4.390671 | Val Cost: 4.502564 | Time: 3.39s\n",
      "Epoch 100/149 | Train Cost: 4.393392 | Val Cost: 4.503057 | Time: 3.39s\n",
      "Epoch 120/149 | Train Cost: 4.394205 | Val Cost: 4.502151 | Time: 3.05s\n",
      "Epoch 140/149 | Train Cost: 4.390096 | Val Cost: 4.500837 | Time: 3.13s\n",
      "Epoch 149/149 | Train Cost: 4.391413 | Val Cost: 4.501188 | Time: 3.16s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 456.35 seconds\n",
      "\n",
      "--- Running Iteration 18/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 54.263382 | Val Cost: 40.369349 | Time: 1.80s\n",
      "Epoch 20/149 | Train Cost: 4.410722 | Val Cost: 4.521744 | Time: 2.33s\n",
      "Epoch 40/149 | Train Cost: 4.393900 | Val Cost: 4.508135 | Time: 1.93s\n",
      "Epoch 60/149 | Train Cost: 4.419141 | Val Cost: 4.504633 | Time: 1.87s\n",
      "Epoch 80/149 | Train Cost: 4.385617 | Val Cost: 4.503004 | Time: 2.09s\n",
      "Epoch 100/149 | Train Cost: 4.392484 | Val Cost: 4.504259 | Time: 2.28s\n",
      "Epoch 120/149 | Train Cost: 4.393123 | Val Cost: 4.502835 | Time: 2.11s\n",
      "Epoch 140/149 | Train Cost: 4.384696 | Val Cost: 4.503127 | Time: 2.11s\n",
      "Epoch 149/149 | Train Cost: 4.387677 | Val Cost: 4.501955 | Time: 2.13s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 305.40 seconds\n",
      "\n",
      "--- Running Iteration 19/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 34.749484 | Val Cost: 9.359417 | Time: 2.71s\n",
      "Epoch 20/149 | Train Cost: 4.404844 | Val Cost: 4.513324 | Time: 3.33s\n",
      "Epoch 40/149 | Train Cost: 4.395538 | Val Cost: 4.505822 | Time: 2.76s\n",
      "Epoch 60/149 | Train Cost: 4.405014 | Val Cost: 4.504183 | Time: 2.65s\n",
      "Epoch 80/149 | Train Cost: 4.391081 | Val Cost: 4.502995 | Time: 2.73s\n",
      "Epoch 100/149 | Train Cost: 4.393576 | Val Cost: 4.503728 | Time: 2.69s\n",
      "Epoch 120/149 | Train Cost: 4.394251 | Val Cost: 4.502610 | Time: 2.69s\n",
      "Epoch 140/149 | Train Cost: 4.390352 | Val Cost: 4.502022 | Time: 2.66s\n",
      "Epoch 149/149 | Train Cost: 4.391606 | Val Cost: 4.501790 | Time: 3.10s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 412.53 seconds\n",
      "\n",
      "--- Running Iteration 20/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 51.456915 | Val Cost: 33.601316 | Time: 1.87s\n",
      "Epoch 20/149 | Train Cost: 4.432353 | Val Cost: 4.541513 | Time: 1.92s\n",
      "Epoch 40/149 | Train Cost: 4.400605 | Val Cost: 4.514238 | Time: 1.91s\n",
      "Epoch 60/149 | Train Cost: 4.422402 | Val Cost: 4.507514 | Time: 1.90s\n",
      "Epoch 80/149 | Train Cost: 4.387665 | Val Cost: 4.504852 | Time: 1.92s\n",
      "Epoch 100/149 | Train Cost: 4.394013 | Val Cost: 4.505168 | Time: 1.92s\n",
      "Epoch 120/149 | Train Cost: 4.394070 | Val Cost: 4.504046 | Time: 1.92s\n",
      "Epoch 140/149 | Train Cost: 4.385575 | Val Cost: 4.504148 | Time: 1.93s\n",
      "Epoch 149/149 | Train Cost: 4.388485 | Val Cost: 4.502922 | Time: 2.35s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 292.65 seconds\n",
      "\n",
      "--- Running Iteration 21/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 24.681388 | Val Cost: 4.655976 | Time: 5.63s\n",
      "Epoch 20/149 | Train Cost: 4.397887 | Val Cost: 4.509408 | Time: 5.73s\n",
      "Epoch 40/149 | Train Cost: 4.397306 | Val Cost: 4.504861 | Time: 6.34s\n",
      "Epoch 60/149 | Train Cost: 4.409633 | Val Cost: 4.515674 | Time: 6.47s\n",
      "Epoch 80/149 | Train Cost: 4.395820 | Val Cost: 4.507205 | Time: 6.59s\n",
      "Epoch 100/149 | Train Cost: 4.399903 | Val Cost: 4.506507 | Time: 6.51s\n",
      "Epoch 120/149 | Train Cost: 4.398928 | Val Cost: 4.502969 | Time: 6.59s\n",
      "Epoch 140/149 | Train Cost: 4.393694 | Val Cost: 4.499884 | Time: 6.60s\n",
      "Epoch 149/149 | Train Cost: 4.395393 | Val Cost: 4.502544 | Time: 6.64s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 962.13 seconds\n",
      "\n",
      "--- Running Iteration 22/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 43.985151 | Val Cost: 8.183403 | Time: 3.51s\n",
      "Epoch 20/149 | Train Cost: 4.392271 | Val Cost: 4.504645 | Time: 3.65s\n",
      "Epoch 40/149 | Train Cost: 4.391394 | Val Cost: 4.505810 | Time: 3.51s\n",
      "Epoch 60/149 | Train Cost: 4.420003 | Val Cost: 4.503392 | Time: 3.52s\n",
      "Epoch 80/149 | Train Cost: 4.386782 | Val Cost: 4.503742 | Time: 4.58s\n",
      "Epoch 100/149 | Train Cost: 4.394118 | Val Cost: 4.503220 | Time: 4.09s\n",
      "Epoch 120/149 | Train Cost: 4.396125 | Val Cost: 4.502547 | Time: 4.07s\n",
      "Epoch 140/149 | Train Cost: 4.386544 | Val Cost: 4.499720 | Time: 4.06s\n",
      "Epoch 149/149 | Train Cost: 4.390041 | Val Cost: 4.501368 | Time: 4.12s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 589.75 seconds\n",
      "\n",
      "--- Running Iteration 23/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 22.778062 | Val Cost: 5.219747 | Time: 6.27s\n",
      "Epoch 20/149 | Train Cost: 4.396889 | Val Cost: 4.506656 | Time: 5.75s\n",
      "Epoch 40/149 | Train Cost: 4.395291 | Val Cost: 4.503844 | Time: 5.72s\n",
      "Epoch 60/149 | Train Cost: 4.406646 | Val Cost: 4.505664 | Time: 5.75s\n",
      "Epoch 80/149 | Train Cost: 4.392910 | Val Cost: 4.504303 | Time: 5.67s\n",
      "Epoch 100/149 | Train Cost: 4.396345 | Val Cost: 4.503943 | Time: 6.39s\n",
      "Epoch 120/149 | Train Cost: 4.397187 | Val Cost: 4.502947 | Time: 5.73s\n",
      "Epoch 140/149 | Train Cost: 4.392989 | Val Cost: 4.500428 | Time: 5.67s\n",
      "Epoch 149/149 | Train Cost: 4.394553 | Val Cost: 4.501634 | Time: 5.94s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 875.43 seconds\n",
      "\n",
      "--- Running Iteration 24/48 ---\n",
      "Config: LR=0.005, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 39.238928 | Val Cost: 9.127470 | Time: 3.74s\n",
      "Epoch 20/149 | Train Cost: 4.400391 | Val Cost: 4.511462 | Time: 3.57s\n",
      "Epoch 40/149 | Train Cost: 4.391922 | Val Cost: 4.505555 | Time: 3.63s\n",
      "Epoch 60/149 | Train Cost: 4.419002 | Val Cost: 4.503585 | Time: 3.63s\n",
      "Epoch 80/149 | Train Cost: 4.386112 | Val Cost: 4.502543 | Time: 3.58s\n",
      "Epoch 100/149 | Train Cost: 4.393207 | Val Cost: 4.503868 | Time: 3.65s\n",
      "Epoch 120/149 | Train Cost: 4.394638 | Val Cost: 4.502369 | Time: 3.62s\n",
      "Epoch 140/149 | Train Cost: 4.386010 | Val Cost: 4.502221 | Time: 3.57s\n",
      "Epoch 149/149 | Train Cost: 4.389151 | Val Cost: 4.501718 | Time: 3.59s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 548.64 seconds\n",
      "\n",
      "--- Running Iteration 25/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 37.822279 | Val Cost: 9.243760 | Time: 2.95s\n",
      "Epoch 20/249 | Train Cost: 4.398410 | Val Cost: 4.507694 | Time: 2.67s\n",
      "Epoch 40/249 | Train Cost: 4.393932 | Val Cost: 4.504267 | Time: 2.97s\n",
      "Epoch 60/249 | Train Cost: 4.404474 | Val Cost: 4.503476 | Time: 3.07s\n",
      "Epoch 80/249 | Train Cost: 4.390671 | Val Cost: 4.502564 | Time: 3.01s\n",
      "Epoch 100/249 | Train Cost: 4.393392 | Val Cost: 4.503057 | Time: 3.06s\n",
      "Epoch 120/249 | Train Cost: 4.394205 | Val Cost: 4.502151 | Time: 3.06s\n",
      "Epoch 140/249 | Train Cost: 4.390096 | Val Cost: 4.500837 | Time: 3.08s\n",
      "Epoch 160/249 | Train Cost: 4.392135 | Val Cost: 4.503122 | Time: 3.09s\n",
      "Epoch 180/249 | Train Cost: 4.392262 | Val Cost: 4.504705 | Time: 3.79s\n",
      "Epoch 200/249 | Train Cost: 4.394237 | Val Cost: 4.503010 | Time: 3.45s\n",
      "Epoch 220/249 | Train Cost: 4.387545 | Val Cost: 4.504140 | Time: 3.47s\n",
      "Epoch 240/249 | Train Cost: 4.397735 | Val Cost: 4.501202 | Time: 3.35s\n",
      "Epoch 249/249 | Train Cost: 4.387797 | Val Cost: 4.501516 | Time: 3.09s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 770.63 seconds\n",
      "\n",
      "--- Running Iteration 26/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 54.263382 | Val Cost: 40.369349 | Time: 1.80s\n",
      "Epoch 20/249 | Train Cost: 4.410722 | Val Cost: 4.521744 | Time: 1.88s\n",
      "Epoch 40/249 | Train Cost: 4.393900 | Val Cost: 4.508135 | Time: 1.87s\n",
      "Epoch 60/249 | Train Cost: 4.419141 | Val Cost: 4.504633 | Time: 1.89s\n",
      "Epoch 80/249 | Train Cost: 4.385617 | Val Cost: 4.503004 | Time: 2.07s\n",
      "Epoch 100/249 | Train Cost: 4.392484 | Val Cost: 4.504259 | Time: 2.62s\n",
      "Epoch 120/249 | Train Cost: 4.393123 | Val Cost: 4.502835 | Time: 2.11s\n",
      "Epoch 140/249 | Train Cost: 4.384696 | Val Cost: 4.503127 | Time: 2.11s\n",
      "Epoch 160/249 | Train Cost: 4.390752 | Val Cost: 4.501878 | Time: 2.36s\n",
      "Epoch 180/249 | Train Cost: 4.390401 | Val Cost: 4.502643 | Time: 2.14s\n",
      "Epoch 200/249 | Train Cost: 4.394584 | Val Cost: 4.501714 | Time: 2.13s\n",
      "Epoch 220/249 | Train Cost: 4.379526 | Val Cost: 4.501551 | Time: 2.20s\n",
      "Epoch 240/249 | Train Cost: 4.404491 | Val Cost: 4.501599 | Time: 2.12s\n",
      "Epoch 249/249 | Train Cost: 4.380570 | Val Cost: 4.500554 | Time: 2.27s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 522.64 seconds\n",
      "\n",
      "--- Running Iteration 27/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 34.749484 | Val Cost: 9.359417 | Time: 2.71s\n",
      "Epoch 20/249 | Train Cost: 4.404844 | Val Cost: 4.513324 | Time: 2.74s\n",
      "Epoch 40/249 | Train Cost: 4.395538 | Val Cost: 4.505822 | Time: 2.67s\n",
      "Epoch 60/249 | Train Cost: 4.405014 | Val Cost: 4.504183 | Time: 2.91s\n",
      "Epoch 80/249 | Train Cost: 4.391081 | Val Cost: 4.502995 | Time: 2.80s\n",
      "Epoch 100/249 | Train Cost: 4.393576 | Val Cost: 4.503728 | Time: 2.65s\n",
      "Epoch 120/249 | Train Cost: 4.394251 | Val Cost: 4.502610 | Time: 2.79s\n",
      "Epoch 140/249 | Train Cost: 4.390352 | Val Cost: 4.502022 | Time: 2.72s\n",
      "Epoch 160/249 | Train Cost: 4.392518 | Val Cost: 4.502796 | Time: 2.83s\n",
      "Epoch 180/249 | Train Cost: 4.392592 | Val Cost: 4.504085 | Time: 2.70s\n",
      "Epoch 200/249 | Train Cost: 4.394488 | Val Cost: 4.502825 | Time: 2.72s\n",
      "Epoch 220/249 | Train Cost: 4.388017 | Val Cost: 4.503006 | Time: 2.71s\n",
      "Epoch 240/249 | Train Cost: 4.398156 | Val Cost: 4.501538 | Time: 2.68s\n",
      "Epoch 249/249 | Train Cost: 4.388338 | Val Cost: 4.501681 | Time: 3.04s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 697.08 seconds\n",
      "\n",
      "--- Running Iteration 28/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 51.456915 | Val Cost: 33.601316 | Time: 1.88s\n",
      "Epoch 20/249 | Train Cost: 4.432353 | Val Cost: 4.541513 | Time: 1.83s\n",
      "Epoch 40/249 | Train Cost: 4.400605 | Val Cost: 4.514238 | Time: 2.10s\n",
      "Epoch 60/249 | Train Cost: 4.422402 | Val Cost: 4.507514 | Time: 1.92s\n",
      "Epoch 80/249 | Train Cost: 4.387665 | Val Cost: 4.504852 | Time: 1.89s\n",
      "Epoch 100/249 | Train Cost: 4.394013 | Val Cost: 4.505168 | Time: 1.89s\n",
      "Epoch 120/249 | Train Cost: 4.394070 | Val Cost: 4.504046 | Time: 1.94s\n",
      "Epoch 140/249 | Train Cost: 4.385575 | Val Cost: 4.504148 | Time: 2.53s\n",
      "Epoch 160/249 | Train Cost: 4.391474 | Val Cost: 4.502800 | Time: 1.90s\n",
      "Epoch 180/249 | Train Cost: 4.391020 | Val Cost: 4.503028 | Time: 1.97s\n",
      "Epoch 200/249 | Train Cost: 4.395097 | Val Cost: 4.502378 | Time: 1.97s\n",
      "Epoch 220/249 | Train Cost: 4.380134 | Val Cost: 4.502253 | Time: 1.94s\n",
      "Epoch 240/249 | Train Cost: 4.405064 | Val Cost: 4.502429 | Time: 1.91s\n",
      "Epoch 249/249 | Train Cost: 4.381095 | Val Cost: 4.501263 | Time: 1.89s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 486.15 seconds\n",
      "\n",
      "--- Running Iteration 29/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 24.681388 | Val Cost: 4.655976 | Time: 5.77s\n",
      "Epoch 20/249 | Train Cost: 4.397887 | Val Cost: 4.509408 | Time: 5.75s\n",
      "Epoch 40/249 | Train Cost: 4.397306 | Val Cost: 4.504861 | Time: 6.46s\n",
      "Epoch 60/249 | Train Cost: 4.409633 | Val Cost: 4.515674 | Time: 6.57s\n",
      "Epoch 80/249 | Train Cost: 4.395820 | Val Cost: 4.507205 | Time: 6.66s\n",
      "Epoch 100/249 | Train Cost: 4.399903 | Val Cost: 4.506507 | Time: 6.68s\n",
      "Epoch 120/249 | Train Cost: 4.398928 | Val Cost: 4.502969 | Time: 6.65s\n",
      "Epoch 140/249 | Train Cost: 4.393694 | Val Cost: 4.499884 | Time: 7.24s\n",
      "Epoch 160/249 | Train Cost: 4.394657 | Val Cost: 4.512353 | Time: 6.69s\n",
      "Epoch 180/249 | Train Cost: 4.394806 | Val Cost: 4.510382 | Time: 6.72s\n",
      "Epoch 200/249 | Train Cost: 4.396687 | Val Cost: 4.505750 | Time: 6.67s\n",
      "Epoch 220/249 | Train Cost: 4.390337 | Val Cost: 4.511044 | Time: 6.64s\n",
      "Epoch 240/249 | Train Cost: 4.399896 | Val Cost: 4.503016 | Time: 6.66s\n",
      "Epoch 249/249 | Train Cost: 4.388002 | Val Cost: 4.501235 | Time: 6.68s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1654.54 seconds\n",
      "\n",
      "--- Running Iteration 30/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 43.985151 | Val Cost: 8.183403 | Time: 3.55s\n",
      "Epoch 20/249 | Train Cost: 4.392271 | Val Cost: 4.504645 | Time: 3.58s\n",
      "Epoch 40/249 | Train Cost: 4.391394 | Val Cost: 4.505810 | Time: 3.60s\n",
      "Epoch 60/249 | Train Cost: 4.420003 | Val Cost: 4.503392 | Time: 3.58s\n",
      "Epoch 80/249 | Train Cost: 4.386782 | Val Cost: 4.503742 | Time: 4.05s\n",
      "Epoch 100/249 | Train Cost: 4.394118 | Val Cost: 4.503220 | Time: 4.13s\n",
      "Epoch 120/249 | Train Cost: 4.396125 | Val Cost: 4.502547 | Time: 4.16s\n",
      "Epoch 140/249 | Train Cost: 4.386544 | Val Cost: 4.499720 | Time: 4.57s\n",
      "Epoch 160/249 | Train Cost: 4.392717 | Val Cost: 4.505099 | Time: 4.16s\n",
      "Epoch 180/249 | Train Cost: 4.393083 | Val Cost: 4.508602 | Time: 4.10s\n",
      "Epoch 200/249 | Train Cost: 4.398122 | Val Cost: 4.504805 | Time: 4.13s\n",
      "Epoch 220/249 | Train Cost: 4.381027 | Val Cost: 4.507855 | Time: 4.14s\n",
      "Epoch 240/249 | Train Cost: 4.406790 | Val Cost: 4.502040 | Time: 4.60s\n",
      "Epoch 249/249 | Train Cost: 4.383174 | Val Cost: 4.502606 | Time: 4.12s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1015.08 seconds\n",
      "\n",
      "--- Running Iteration 31/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 22.778062 | Val Cost: 5.219747 | Time: 5.73s\n",
      "Epoch 20/249 | Train Cost: 4.396889 | Val Cost: 4.506656 | Time: 5.80s\n",
      "Epoch 40/249 | Train Cost: 4.395291 | Val Cost: 4.503844 | Time: 5.72s\n",
      "Epoch 60/249 | Train Cost: 4.406646 | Val Cost: 4.505664 | Time: 5.71s\n",
      "Epoch 80/249 | Train Cost: 4.392910 | Val Cost: 4.504303 | Time: 5.65s\n",
      "Epoch 100/249 | Train Cost: 4.396345 | Val Cost: 4.503943 | Time: 5.72s\n",
      "Epoch 120/249 | Train Cost: 4.397187 | Val Cost: 4.502947 | Time: 6.13s\n",
      "Epoch 140/249 | Train Cost: 4.392989 | Val Cost: 4.500428 | Time: 5.65s\n",
      "Epoch 160/249 | Train Cost: 4.394949 | Val Cost: 4.506223 | Time: 5.77s\n",
      "Epoch 180/249 | Train Cost: 4.395494 | Val Cost: 4.507220 | Time: 5.72s\n",
      "Epoch 200/249 | Train Cost: 4.397800 | Val Cost: 4.505316 | Time: 5.69s\n",
      "Epoch 220/249 | Train Cost: 4.391376 | Val Cost: 4.509046 | Time: 6.25s\n",
      "Epoch 240/249 | Train Cost: 4.401843 | Val Cost: 4.502780 | Time: 5.70s\n",
      "Epoch 249/249 | Train Cost: 4.391562 | Val Cost: 4.502693 | Time: 5.75s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1455.25 seconds\n",
      "\n",
      "--- Running Iteration 32/48 ---\n",
      "Config: LR=0.005, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.005, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 39.238928 | Val Cost: 9.127470 | Time: 3.59s\n",
      "Epoch 20/249 | Train Cost: 4.400391 | Val Cost: 4.511462 | Time: 3.59s\n",
      "Epoch 40/249 | Train Cost: 4.391922 | Val Cost: 4.505555 | Time: 3.65s\n",
      "Epoch 60/249 | Train Cost: 4.419002 | Val Cost: 4.503585 | Time: 3.59s\n",
      "Epoch 80/249 | Train Cost: 4.386112 | Val Cost: 4.502543 | Time: 3.58s\n",
      "Epoch 100/249 | Train Cost: 4.393207 | Val Cost: 4.503868 | Time: 3.65s\n",
      "Epoch 120/249 | Train Cost: 4.394638 | Val Cost: 4.502369 | Time: 3.76s\n",
      "Epoch 140/249 | Train Cost: 4.386010 | Val Cost: 4.502221 | Time: 3.59s\n",
      "Epoch 160/249 | Train Cost: 4.392276 | Val Cost: 4.502291 | Time: 3.66s\n",
      "Epoch 180/249 | Train Cost: 4.392221 | Val Cost: 4.504439 | Time: 3.65s\n",
      "Epoch 200/249 | Train Cost: 4.396763 | Val Cost: 4.502514 | Time: 4.02s\n",
      "Epoch 220/249 | Train Cost: 4.381329 | Val Cost: 4.502218 | Time: 3.62s\n",
      "Epoch 240/249 | Train Cost: 4.406564 | Val Cost: 4.501683 | Time: 3.60s\n",
      "Epoch 249/249 | Train Cost: 4.382720 | Val Cost: 4.501261 | Time: 3.70s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 918.71 seconds\n",
      "\n",
      "--- Running Iteration 33/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 59.348651 | Val Cost: 55.687274 | Time: 2.67s\n",
      "Epoch 20/149 | Train Cost: 4.518403 | Val Cost: 4.617927 | Time: 2.62s\n",
      "Epoch 40/149 | Train Cost: 4.425702 | Val Cost: 4.534782 | Time: 3.18s\n",
      "Epoch 60/149 | Train Cost: 4.418655 | Val Cost: 4.517276 | Time: 3.24s\n",
      "Epoch 80/149 | Train Cost: 4.398863 | Val Cost: 4.510635 | Time: 3.24s\n",
      "Epoch 100/149 | Train Cost: 4.398483 | Val Cost: 4.507990 | Time: 3.26s\n",
      "Epoch 120/149 | Train Cost: 4.396782 | Val Cost: 4.506238 | Time: 3.26s\n",
      "Epoch 140/149 | Train Cost: 4.392295 | Val Cost: 4.505442 | Time: 3.27s\n",
      "Epoch 149/149 | Train Cost: 4.393092 | Val Cost: 4.504573 | Time: 3.33s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 475.20 seconds\n",
      "\n",
      "--- Running Iteration 34/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 61.043147 | Val Cost: 59.407833 | Time: 1.82s\n",
      "Epoch 20/149 | Train Cost: 4.942375 | Val Cost: 5.010644 | Time: 1.87s\n",
      "Epoch 40/149 | Train Cost: 4.516533 | Val Cost: 4.622537 | Time: 1.98s\n",
      "Epoch 60/149 | Train Cost: 4.475110 | Val Cost: 4.557714 | Time: 2.05s\n",
      "Epoch 80/149 | Train Cost: 4.418610 | Val Cost: 4.534777 | Time: 2.22s\n",
      "Epoch 100/149 | Train Cost: 4.414919 | Val Cost: 4.523675 | Time: 2.23s\n",
      "Epoch 120/149 | Train Cost: 4.408825 | Val Cost: 4.517572 | Time: 2.26s\n",
      "Epoch 140/149 | Train Cost: 4.396608 | Val Cost: 4.513797 | Time: 2.22s\n",
      "Epoch 149/149 | Train Cost: 4.398170 | Val Cost: 4.512072 | Time: 2.21s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 319.34 seconds\n",
      "\n",
      "--- Running Iteration 35/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 58.376120 | Val Cost: 53.455070 | Time: 2.67s\n",
      "Epoch 20/149 | Train Cost: 4.622249 | Val Cost: 4.714348 | Time: 2.81s\n",
      "Epoch 40/149 | Train Cost: 4.458395 | Val Cost: 4.564966 | Time: 2.68s\n",
      "Epoch 60/149 | Train Cost: 4.435143 | Val Cost: 4.532713 | Time: 2.73s\n",
      "Epoch 80/149 | Train Cost: 4.409126 | Val Cost: 4.520223 | Time: 2.66s\n",
      "Epoch 100/149 | Train Cost: 4.405629 | Val Cost: 4.514301 | Time: 2.75s\n",
      "Epoch 120/149 | Train Cost: 4.402034 | Val Cost: 4.510824 | Time: 2.64s\n",
      "Epoch 140/149 | Train Cost: 4.396386 | Val Cost: 4.508859 | Time: 2.69s\n",
      "Epoch 149/149 | Train Cost: 4.396761 | Val Cost: 4.507744 | Time: 2.69s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 408.59 seconds\n",
      "\n",
      "--- Running Iteration 36/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 60.615511 | Val Cost: 58.486941 | Time: 2.17s\n",
      "Epoch 20/149 | Train Cost: 5.264094 | Val Cost: 5.314652 | Time: 1.90s\n",
      "Epoch 40/149 | Train Cost: 4.622181 | Val Cost: 4.722139 | Time: 1.93s\n",
      "Epoch 60/149 | Train Cost: 4.527893 | Val Cost: 4.607637 | Time: 1.90s\n",
      "Epoch 80/149 | Train Cost: 4.451406 | Val Cost: 4.565396 | Time: 1.94s\n",
      "Epoch 100/149 | Train Cost: 4.437370 | Val Cost: 4.544725 | Time: 2.63s\n",
      "Epoch 120/149 | Train Cost: 4.425530 | Val Cost: 4.533083 | Time: 1.89s\n",
      "Epoch 140/149 | Train Cost: 4.409612 | Val Cost: 4.525715 | Time: 1.88s\n",
      "Epoch 149/149 | Train Cost: 4.409716 | Val Cost: 4.522882 | Time: 1.87s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 289.24 seconds\n",
      "\n",
      "--- Running Iteration 37/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 57.598871 | Val Cost: 49.809817 | Time: 5.94s\n",
      "Epoch 20/149 | Train Cost: 4.401730 | Val Cost: 4.510686 | Time: 5.61s\n",
      "Epoch 40/149 | Train Cost: 4.394571 | Val Cost: 4.505112 | Time: 6.82s\n",
      "Epoch 60/149 | Train Cost: 4.404520 | Val Cost: 4.503772 | Time: 6.95s\n",
      "Epoch 80/149 | Train Cost: 4.390742 | Val Cost: 4.502643 | Time: 7.18s\n",
      "Epoch 100/149 | Train Cost: 4.393323 | Val Cost: 4.503359 | Time: 6.97s\n",
      "Epoch 120/149 | Train Cost: 4.394094 | Val Cost: 4.502312 | Time: 6.97s\n",
      "Epoch 140/149 | Train Cost: 4.390210 | Val Cost: 4.501619 | Time: 7.03s\n",
      "Epoch 149/149 | Train Cost: 4.391492 | Val Cost: 4.501461 | Time: 7.04s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1013.67 seconds\n",
      "\n",
      "--- Running Iteration 38/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 60.544109 | Val Cost: 58.126842 | Time: 3.63s\n",
      "Epoch 20/149 | Train Cost: 4.422789 | Val Cost: 4.534121 | Time: 3.66s\n",
      "Epoch 40/149 | Train Cost: 4.397342 | Val Cost: 4.511486 | Time: 3.72s\n",
      "Epoch 60/149 | Train Cost: 4.420875 | Val Cost: 4.506155 | Time: 3.61s\n",
      "Epoch 80/149 | Train Cost: 4.386702 | Val Cost: 4.504104 | Time: 4.31s\n",
      "Epoch 100/149 | Train Cost: 4.393323 | Val Cost: 4.504645 | Time: 4.35s\n",
      "Epoch 120/149 | Train Cost: 4.393603 | Val Cost: 4.503661 | Time: 4.35s\n",
      "Epoch 140/149 | Train Cost: 4.385144 | Val Cost: 4.503770 | Time: 4.54s\n",
      "Epoch 149/149 | Train Cost: 4.388120 | Val Cost: 4.502602 | Time: 4.34s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 616.27 seconds\n",
      "\n",
      "--- Running Iteration 39/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 54.868497 | Val Cost: 42.274839 | Time: 5.75s\n",
      "Epoch 20/149 | Train Cost: 4.449080 | Val Cost: 4.554741 | Time: 6.06s\n",
      "Epoch 40/149 | Train Cost: 4.408432 | Val Cost: 4.518063 | Time: 5.80s\n",
      "Epoch 60/149 | Train Cost: 4.410733 | Val Cost: 4.509524 | Time: 5.74s\n",
      "Epoch 80/149 | Train Cost: 4.394410 | Val Cost: 4.506092 | Time: 5.89s\n",
      "Epoch 100/149 | Train Cost: 4.395664 | Val Cost: 4.505579 | Time: 5.86s\n",
      "Epoch 120/149 | Train Cost: 4.395186 | Val Cost: 4.504316 | Time: 6.05s\n",
      "Epoch 140/149 | Train Cost: 4.391258 | Val Cost: 4.504039 | Time: 5.81s\n",
      "Epoch 149/149 | Train Cost: 4.392295 | Val Cost: 4.503150 | Time: 5.69s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 880.30 seconds\n",
      "\n",
      "--- Running Iteration 40/48 ---\n",
      "Config: LR=0.001, Epochs=150, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 150 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/149 | Train Cost: 59.586640 | Val Cost: 55.922765 | Time: 3.57s\n",
      "Epoch 20/149 | Train Cost: 4.581268 | Val Cost: 4.680172 | Time: 3.64s\n",
      "Epoch 40/149 | Train Cost: 4.445846 | Val Cost: 4.556904 | Time: 3.63s\n",
      "Epoch 60/149 | Train Cost: 4.444513 | Val Cost: 4.528616 | Time: 3.69s\n",
      "Epoch 80/149 | Train Cost: 4.401214 | Val Cost: 4.517648 | Time: 3.62s\n",
      "Epoch 100/149 | Train Cost: 4.403373 | Val Cost: 4.512536 | Time: 4.10s\n",
      "Epoch 120/149 | Train Cost: 4.400552 | Val Cost: 4.509709 | Time: 3.68s\n",
      "Epoch 140/149 | Train Cost: 4.390538 | Val Cost: 4.508111 | Time: 3.63s\n",
      "Epoch 149/149 | Train Cost: 4.392876 | Val Cost: 4.506837 | Time: 3.63s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 554.13 seconds\n",
      "\n",
      "--- Running Iteration 41/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 59.348651 | Val Cost: 55.687274 | Time: 2.68s\n",
      "Epoch 20/249 | Train Cost: 4.518403 | Val Cost: 4.617927 | Time: 2.78s\n",
      "Epoch 40/249 | Train Cost: 4.425702 | Val Cost: 4.534782 | Time: 3.21s\n",
      "Epoch 60/249 | Train Cost: 4.418655 | Val Cost: 4.517276 | Time: 3.30s\n",
      "Epoch 80/249 | Train Cost: 4.398863 | Val Cost: 4.510635 | Time: 3.29s\n",
      "Epoch 100/249 | Train Cost: 4.398483 | Val Cost: 4.507990 | Time: 3.26s\n",
      "Epoch 120/249 | Train Cost: 4.396782 | Val Cost: 4.506238 | Time: 3.25s\n",
      "Epoch 140/249 | Train Cost: 4.392295 | Val Cost: 4.505442 | Time: 3.20s\n",
      "Epoch 160/249 | Train Cost: 4.393873 | Val Cost: 4.504356 | Time: 3.31s\n",
      "Epoch 180/249 | Train Cost: 4.393225 | Val Cost: 4.503991 | Time: 3.27s\n",
      "Epoch 200/249 | Train Cost: 4.394478 | Val Cost: 4.503523 | Time: 3.21s\n",
      "Epoch 220/249 | Train Cost: 4.388247 | Val Cost: 4.503079 | Time: 3.65s\n",
      "Epoch 240/249 | Train Cost: 4.397980 | Val Cost: 4.503046 | Time: 3.26s\n",
      "Epoch 249/249 | Train Cost: 4.388271 | Val Cost: 4.502484 | Time: 3.29s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 807.63 seconds\n",
      "\n",
      "--- Running Iteration 42/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 61.043147 | Val Cost: 59.407833 | Time: 1.87s\n",
      "Epoch 20/249 | Train Cost: 4.942375 | Val Cost: 5.010644 | Time: 1.82s\n",
      "Epoch 40/249 | Train Cost: 4.516533 | Val Cost: 4.622537 | Time: 2.22s\n",
      "Epoch 60/249 | Train Cost: 4.475110 | Val Cost: 4.557714 | Time: 1.89s\n",
      "Epoch 80/249 | Train Cost: 4.418610 | Val Cost: 4.534777 | Time: 2.23s\n",
      "Epoch 100/249 | Train Cost: 4.414919 | Val Cost: 4.523675 | Time: 2.38s\n",
      "Epoch 120/249 | Train Cost: 4.408825 | Val Cost: 4.517572 | Time: 2.25s\n",
      "Epoch 140/249 | Train Cost: 4.396608 | Val Cost: 4.513797 | Time: 2.20s\n",
      "Epoch 160/249 | Train Cost: 4.400011 | Val Cost: 4.510673 | Time: 2.31s\n",
      "Epoch 180/249 | Train Cost: 4.397605 | Val Cost: 4.509026 | Time: 2.24s\n",
      "Epoch 200/249 | Train Cost: 4.400369 | Val Cost: 4.507631 | Time: 2.26s\n",
      "Epoch 220/249 | Train Cost: 4.384555 | Val Cost: 4.506675 | Time: 2.23s\n",
      "Epoch 240/249 | Train Cost: 4.408827 | Val Cost: 4.506178 | Time: 2.22s\n",
      "Epoch 249/249 | Train Cost: 4.384364 | Val Cost: 4.505471 | Time: 2.24s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 545.44 seconds\n",
      "\n",
      "--- Running Iteration 43/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 58.376120 | Val Cost: 53.455070 | Time: 2.69s\n",
      "Epoch 20/249 | Train Cost: 4.622249 | Val Cost: 4.714348 | Time: 2.76s\n",
      "Epoch 40/249 | Train Cost: 4.458395 | Val Cost: 4.564966 | Time: 2.67s\n",
      "Epoch 60/249 | Train Cost: 4.435143 | Val Cost: 4.532713 | Time: 2.76s\n",
      "Epoch 80/249 | Train Cost: 4.409126 | Val Cost: 4.520223 | Time: 2.69s\n",
      "Epoch 100/249 | Train Cost: 4.405629 | Val Cost: 4.514301 | Time: 2.72s\n",
      "Epoch 120/249 | Train Cost: 4.402034 | Val Cost: 4.510824 | Time: 2.86s\n",
      "Epoch 140/249 | Train Cost: 4.396386 | Val Cost: 4.508859 | Time: 2.70s\n",
      "Epoch 160/249 | Train Cost: 4.397177 | Val Cost: 4.507093 | Time: 2.69s\n",
      "Epoch 180/249 | Train Cost: 4.395931 | Val Cost: 4.506192 | Time: 2.72s\n",
      "Epoch 200/249 | Train Cost: 4.396761 | Val Cost: 4.505436 | Time: 2.70s\n",
      "Epoch 220/249 | Train Cost: 4.390238 | Val Cost: 4.504787 | Time: 2.68s\n",
      "Epoch 240/249 | Train Cost: 4.399724 | Val Cost: 4.504597 | Time: 2.70s\n",
      "Epoch 249/249 | Train Cost: 4.389918 | Val Cost: 4.504059 | Time: 3.42s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 689.69 seconds\n",
      "\n",
      "--- Running Iteration 44/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 60.615511 | Val Cost: 58.486941 | Time: 1.83s\n",
      "Epoch 20/249 | Train Cost: 5.264094 | Val Cost: 5.314652 | Time: 1.89s\n",
      "Epoch 40/249 | Train Cost: 4.622181 | Val Cost: 4.722139 | Time: 1.93s\n",
      "Epoch 60/249 | Train Cost: 4.527893 | Val Cost: 4.607637 | Time: 1.95s\n",
      "Epoch 80/249 | Train Cost: 4.451406 | Val Cost: 4.565396 | Time: 1.91s\n",
      "Epoch 100/249 | Train Cost: 4.437370 | Val Cost: 4.544725 | Time: 1.91s\n",
      "Epoch 120/249 | Train Cost: 4.425530 | Val Cost: 4.533083 | Time: 1.93s\n",
      "Epoch 140/249 | Train Cost: 4.409612 | Val Cost: 4.525715 | Time: 1.92s\n",
      "Epoch 160/249 | Train Cost: 4.410456 | Val Cost: 4.520220 | Time: 1.91s\n",
      "Epoch 180/249 | Train Cost: 4.406114 | Val Cost: 4.516826 | Time: 2.26s\n",
      "Epoch 200/249 | Train Cost: 4.407618 | Val Cost: 4.514102 | Time: 1.95s\n",
      "Epoch 220/249 | Train Cost: 4.390662 | Val Cost: 4.512124 | Time: 1.94s\n",
      "Epoch 240/249 | Train Cost: 4.414170 | Val Cost: 4.510753 | Time: 1.98s\n",
      "Epoch 249/249 | Train Cost: 4.389407 | Val Cost: 4.509893 | Time: 1.93s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 490.16 seconds\n",
      "\n",
      "--- Running Iteration 45/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 57.598871 | Val Cost: 49.809817 | Time: 5.80s\n",
      "Epoch 20/249 | Train Cost: 4.401730 | Val Cost: 4.510686 | Time: 6.09s\n",
      "Epoch 40/249 | Train Cost: 4.394571 | Val Cost: 4.505112 | Time: 7.36s\n",
      "Epoch 60/249 | Train Cost: 4.404520 | Val Cost: 4.503772 | Time: 7.05s\n",
      "Epoch 80/249 | Train Cost: 4.390742 | Val Cost: 4.502643 | Time: 7.46s\n",
      "Epoch 100/249 | Train Cost: 4.393323 | Val Cost: 4.503359 | Time: 7.08s\n",
      "Epoch 120/249 | Train Cost: 4.394094 | Val Cost: 4.502312 | Time: 7.30s\n",
      "Epoch 140/249 | Train Cost: 4.390210 | Val Cost: 4.501619 | Time: 7.17s\n",
      "Epoch 160/249 | Train Cost: 4.392433 | Val Cost: 4.502435 | Time: 7.79s\n",
      "Epoch 180/249 | Train Cost: 4.392538 | Val Cost: 4.503695 | Time: 7.15s\n",
      "Epoch 200/249 | Train Cost: 4.394506 | Val Cost: 4.502427 | Time: 7.51s\n",
      "Epoch 220/249 | Train Cost: 4.388034 | Val Cost: 4.502678 | Time: 7.11s\n",
      "Epoch 240/249 | Train Cost: 4.398243 | Val Cost: 4.501164 | Time: 7.62s\n",
      "Epoch 249/249 | Train Cost: 4.388448 | Val Cost: 4.501256 | Time: 7.49s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1763.09 seconds\n",
      "\n",
      "--- Running Iteration 46/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 256, 128, 90], Act=relu, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 60.544109 | Val Cost: 58.126842 | Time: 3.62s\n",
      "Epoch 20/249 | Train Cost: 4.422789 | Val Cost: 4.534121 | Time: 3.59s\n",
      "Epoch 40/249 | Train Cost: 4.397342 | Val Cost: 4.511486 | Time: 3.54s\n",
      "Epoch 60/249 | Train Cost: 4.420875 | Val Cost: 4.506155 | Time: 3.96s\n",
      "Epoch 80/249 | Train Cost: 4.386702 | Val Cost: 4.504104 | Time: 4.37s\n",
      "Epoch 100/249 | Train Cost: 4.393323 | Val Cost: 4.504645 | Time: 4.46s\n",
      "Epoch 120/249 | Train Cost: 4.393603 | Val Cost: 4.503661 | Time: 4.41s\n",
      "Epoch 140/249 | Train Cost: 4.385144 | Val Cost: 4.503770 | Time: 4.42s\n",
      "Epoch 160/249 | Train Cost: 4.391131 | Val Cost: 4.502451 | Time: 4.39s\n",
      "Epoch 180/249 | Train Cost: 4.390697 | Val Cost: 4.502689 | Time: 4.38s\n",
      "Epoch 200/249 | Train Cost: 4.394861 | Val Cost: 4.502035 | Time: 4.41s\n",
      "Epoch 220/249 | Train Cost: 4.379881 | Val Cost: 4.501928 | Time: 4.40s\n",
      "Epoch 240/249 | Train Cost: 4.404875 | Val Cost: 4.502093 | Time: 5.03s\n",
      "Epoch 249/249 | Train Cost: 4.380934 | Val Cost: 4.500906 | Time: 4.38s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1068.61 seconds\n",
      "\n",
      "--- Running Iteration 47/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=32\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=32, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 54.868497 | Val Cost: 42.274839 | Time: 5.88s\n",
      "Epoch 20/249 | Train Cost: 4.449080 | Val Cost: 4.554741 | Time: 5.94s\n",
      "Epoch 40/249 | Train Cost: 4.408432 | Val Cost: 4.518063 | Time: 5.83s\n",
      "Epoch 60/249 | Train Cost: 4.410733 | Val Cost: 4.509524 | Time: 5.88s\n",
      "Epoch 80/249 | Train Cost: 4.394410 | Val Cost: 4.506092 | Time: 5.93s\n",
      "Epoch 100/249 | Train Cost: 4.395664 | Val Cost: 4.505579 | Time: 6.23s\n",
      "Epoch 120/249 | Train Cost: 4.395186 | Val Cost: 4.504316 | Time: 5.88s\n",
      "Epoch 140/249 | Train Cost: 4.391258 | Val Cost: 4.504039 | Time: 5.90s\n",
      "Epoch 160/249 | Train Cost: 4.393279 | Val Cost: 4.503247 | Time: 5.86s\n",
      "Epoch 180/249 | Train Cost: 4.393045 | Val Cost: 4.503336 | Time: 6.39s\n",
      "Epoch 200/249 | Train Cost: 4.394635 | Val Cost: 4.502830 | Time: 5.95s\n",
      "Epoch 220/249 | Train Cost: 4.388481 | Val Cost: 4.502473 | Time: 5.91s\n",
      "Epoch 240/249 | Train Cost: 4.398431 | Val Cost: 4.502310 | Time: 5.97s\n",
      "Epoch 249/249 | Train Cost: 4.388752 | Val Cost: 4.501788 | Time: 5.83s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 1497.57 seconds\n",
      "\n",
      "--- Running Iteration 48/48 ---\n",
      "Config: LR=0.001, Epochs=250, Arch=[5000, 256, 128, 90], Act=tanh, Opt=minibatch+Momentum, Batch=64\n",
      "Starting training: 250 epochs, LR=0.001, BatchSize=64, Optimizer=minibatch w/ Momentum=0.9\n",
      "Epoch 0/249 | Train Cost: 59.586640 | Val Cost: 55.922765 | Time: 4.38s\n",
      "Epoch 20/249 | Train Cost: 4.581268 | Val Cost: 4.680172 | Time: 3.72s\n",
      "Epoch 40/249 | Train Cost: 4.445846 | Val Cost: 4.556904 | Time: 3.68s\n",
      "Epoch 60/249 | Train Cost: 4.444513 | Val Cost: 4.528616 | Time: 4.22s\n",
      "Epoch 80/249 | Train Cost: 4.401214 | Val Cost: 4.517648 | Time: 3.65s\n",
      "Epoch 100/249 | Train Cost: 4.403373 | Val Cost: 4.512536 | Time: 3.70s\n",
      "Epoch 120/249 | Train Cost: 4.400552 | Val Cost: 4.509709 | Time: 3.69s\n",
      "Epoch 140/249 | Train Cost: 4.390538 | Val Cost: 4.508111 | Time: 3.67s\n",
      "Epoch 160/249 | Train Cost: 4.395335 | Val Cost: 4.506224 | Time: 3.67s\n",
      "Epoch 180/249 | Train Cost: 4.394001 | Val Cost: 4.505484 | Time: 3.87s\n",
      "Epoch 200/249 | Train Cost: 4.397502 | Val Cost: 4.504742 | Time: 3.66s\n",
      "Epoch 220/249 | Train Cost: 4.382276 | Val Cost: 4.504300 | Time: 3.73s\n",
      "Epoch 240/249 | Train Cost: 4.406906 | Val Cost: 4.504267 | Time: 4.25s\n",
      "Epoch 249/249 | Train Cost: 4.382694 | Val Cost: 4.503376 | Time: 4.50s\n",
      "Test Set Results:\n",
      "  Accuracy(Subset) = 0.0000\n",
      "  Hamming Loss     = 0.0138\n",
      "  F1 Micro         = 0.0000\n",
      "  F1 Macro         = 0.0000\n",
      "  F1 Samples       = 0.0000\n",
      "Training Time: 937.48 seconds\n",
      "\n",
      "\n",
      "--- Experiment Results ---\n",
      "\n",
      "Top 5 Configurations (Sorted by F1 Micro Score):\n",
      "   learning_rate  epochs activation           optimizer  batch_size  \\\n",
      "0          0.010     250       relu  minibatch+Momentum          32   \n",
      "1          0.005     250       relu  minibatch+Momentum          64   \n",
      "2          0.005     250       tanh  minibatch+Momentum          64   \n",
      "3          0.005     250       relu  minibatch+Momentum          32   \n",
      "4          0.005     250       relu  minibatch+Momentum          64   \n",
      "\n",
      "           architecture  f1_micro  accuracy  hamming_loss  f1_samples  \n",
      "0  [5000, 256, 128, 90]  0.001601  0.000994      0.013768    0.000994  \n",
      "1       [5000, 128, 90]  0.000000  0.000000      0.013779    0.000000  \n",
      "2       [5000, 128, 90]  0.000000  0.000000      0.013779    0.000000  \n",
      "3  [5000, 256, 128, 90]  0.000000  0.000000      0.013779    0.000000  \n",
      "4  [5000, 256, 128, 90]  0.000000  0.000000      0.013779    0.000000  \n",
      "\n",
      "Full Results Table (Sorted by F1 Micro Score):\n",
      "    learning_rate  epochs activation           optimizer  batch_size  \\\n",
      "0           0.010     250       relu  minibatch+Momentum          32   \n",
      "1           0.005     250       relu  minibatch+Momentum          64   \n",
      "2           0.005     250       tanh  minibatch+Momentum          64   \n",
      "3           0.005     250       relu  minibatch+Momentum          32   \n",
      "4           0.005     250       relu  minibatch+Momentum          64   \n",
      "5           0.005     250       tanh  minibatch+Momentum          32   \n",
      "6           0.005     250       tanh  minibatch+Momentum          64   \n",
      "7           0.001     150       relu  minibatch+Momentum          32   \n",
      "8           0.001     150       relu  minibatch+Momentum          64   \n",
      "9           0.001     150       tanh  minibatch+Momentum          32   \n",
      "10          0.001     150       tanh  minibatch+Momentum          64   \n",
      "11          0.001     150       relu  minibatch+Momentum          32   \n",
      "12          0.010     150       relu  minibatch+Momentum          32   \n",
      "13          0.001     150       relu  minibatch+Momentum          64   \n",
      "14          0.001     150       tanh  minibatch+Momentum          32   \n",
      "15          0.001     150       tanh  minibatch+Momentum          64   \n",
      "16          0.001     250       relu  minibatch+Momentum          32   \n",
      "17          0.001     250       relu  minibatch+Momentum          64   \n",
      "18          0.001     250       tanh  minibatch+Momentum          32   \n",
      "19          0.001     250       tanh  minibatch+Momentum          64   \n",
      "20          0.001     250       relu  minibatch+Momentum          32   \n",
      "21          0.001     250       relu  minibatch+Momentum          64   \n",
      "22          0.001     250       tanh  minibatch+Momentum          32   \n",
      "23          0.005     250       tanh  minibatch+Momentum          32   \n",
      "24          0.005     250       relu  minibatch+Momentum          32   \n",
      "25          0.010     150       relu  minibatch+Momentum          64   \n",
      "26          0.010     250       tanh  minibatch+Momentum          64   \n",
      "27          0.010     150       tanh  minibatch+Momentum          32   \n",
      "28          0.010     150       tanh  minibatch+Momentum          64   \n",
      "29          0.010     150       relu  minibatch+Momentum          32   \n",
      "30          0.010     150       relu  minibatch+Momentum          64   \n",
      "31          0.010     150       tanh  minibatch+Momentum          32   \n",
      "32          0.010     150       tanh  minibatch+Momentum          64   \n",
      "33          0.010     250       relu  minibatch+Momentum          32   \n",
      "34          0.010     250       relu  minibatch+Momentum          64   \n",
      "35          0.010     250       tanh  minibatch+Momentum          32   \n",
      "36          0.010     250       relu  minibatch+Momentum          64   \n",
      "37          0.005     150       tanh  minibatch+Momentum          64   \n",
      "38          0.010     250       tanh  minibatch+Momentum          32   \n",
      "39          0.010     250       tanh  minibatch+Momentum          64   \n",
      "40          0.005     150       relu  minibatch+Momentum          32   \n",
      "41          0.005     150       relu  minibatch+Momentum          64   \n",
      "42          0.005     150       tanh  minibatch+Momentum          32   \n",
      "43          0.005     150       tanh  minibatch+Momentum          64   \n",
      "44          0.005     150       relu  minibatch+Momentum          32   \n",
      "45          0.005     150       relu  minibatch+Momentum          64   \n",
      "46          0.005     150       tanh  minibatch+Momentum          32   \n",
      "47          0.001     250       tanh  minibatch+Momentum          64   \n",
      "\n",
      "            architecture  f1_micro  accuracy  hamming_loss  f1_macro  \\\n",
      "0   [5000, 256, 128, 90]  0.001601  0.000994      0.013768  0.000092   \n",
      "1        [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "2        [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "3   [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "4   [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "5   [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "6   [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "7        [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "8        [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "9        [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "10       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "11  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "12       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "13  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "14  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "15  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "16       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "17       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "18       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "19       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "20  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "21  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "22  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "23       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "24       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "25       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "26       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "27       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "28       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "29  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "30  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "31  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "32  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "33       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "34       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "35       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "36  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "37  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "38  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "39  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "40       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "41       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "42       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "43       [5000, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "44  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "45  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "46  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "47  [5000, 256, 128, 90]  0.000000  0.000000      0.013779  0.000000   \n",
      "\n",
      "    f1_samples  training_time  \n",
      "0     0.000994    1606.209391  \n",
      "1     0.000000     522.641352  \n",
      "2     0.000000     486.148431  \n",
      "3     0.000000    1654.535756  \n",
      "4     0.000000    1015.079460  \n",
      "5     0.000000    1455.246879  \n",
      "6     0.000000     918.710828  \n",
      "7     0.000000     475.201164  \n",
      "8     0.000000     319.340509  \n",
      "9     0.000000     408.585295  \n",
      "10    0.000000     289.244900  \n",
      "11    0.000000    1013.671994  \n",
      "12    0.000000     483.723368  \n",
      "13    0.000000     616.270051  \n",
      "14    0.000000     880.301186  \n",
      "15    0.000000     554.126479  \n",
      "16    0.000000     807.625740  \n",
      "17    0.000000     545.442790  \n",
      "18    0.000000     689.693713  \n",
      "19    0.000000     490.159563  \n",
      "20    0.000000    1763.090039  \n",
      "21    0.000000    1068.613088  \n",
      "22    0.000000    1497.572176  \n",
      "23    0.000000     697.076551  \n",
      "24    0.000000     770.627494  \n",
      "25    0.000000     311.291292  \n",
      "26    0.000000     506.117792  \n",
      "27    0.000000     454.543442  \n",
      "28    0.000000     314.143975  \n",
      "29    0.000000     951.577352  \n",
      "30    0.000000     575.964452  \n",
      "31    0.000000     877.444346  \n",
      "32    0.000000     559.799608  \n",
      "33    0.000000     744.288275  \n",
      "34    0.000000     505.012609  \n",
      "35    0.000000     700.032282  \n",
      "36    0.000000     968.315063  \n",
      "37    0.000000     548.639908  \n",
      "38    0.000000    1482.944646  \n",
      "39    0.000000     914.745400  \n",
      "40    0.000000     456.348588  \n",
      "41    0.000000     305.400185  \n",
      "42    0.000000     412.525101  \n",
      "43    0.000000     292.650529  \n",
      "44    0.000000     962.127397  \n",
      "45    0.000000     589.745685  \n",
      "46    0.000000     875.425077  \n",
      "47    0.000000     937.482085  \n",
      "\n",
      "Best Configuration Found (based on F1 Micro):\n",
      "  Learning Rate: 0.01\n",
      "  Epochs: 250\n",
      "  Architecture: [5000, 256, 128, 90]\n",
      "  Hidden Activation: relu\n",
      "  Optimizer: minibatch+Momentum\n",
      "  Batch Size: 32\n",
      "  Test F1 Micro: 0.0016\n",
      "  Test Accuracy (Subset): 0.0010\n",
      "  Test Hamming Loss: 0.0138\n",
      "  Test F1 Macro: 0.0001\n",
      "  Test F1 Samples: 0.0010\n",
      "\n",
      "--- Plotting Training Curves for Best Configuration ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re # Regular expressions for cleaning\n",
    "import time\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Helpful for multi-label format\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score # Multi-label metrics\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Constants ---\n",
    "# Using common English stop words (can be expanded)\n",
    "STOP_WORDS = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were',\n",
    "    'will', 'with', 'i', 'you', 'your', 's', 't', 'm', 're', 'll', 'd', 've',\n",
    "    'about', 'k', 'co', 'inc', 'ltd', 'mln', 'pct', 'dlrs', 'said', 'cts' # Added domain-specific noise words\n",
    "])\n",
    "MAX_FEATURES = 5000 # Limit TF-IDF features\n",
    "\n",
    "# --- 2.3.1 Data Preprocessing ---\n",
    "print(\"--- Data Preprocessing ---\")\n",
    "\n",
    "# Load Data\n",
    "try:\n",
    "    df_train_orig = pd.read_csv('train.csv')\n",
    "    df_test_orig = pd.read_csv('test.csv')\n",
    "    print(f\"Loaded train.csv: {df_train_orig.shape}\")\n",
    "    print(f\"Loaded test.csv: {df_test_orig.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found.\")\n",
    "    try:\n",
    "        df_train_orig = pd.read_csv('/kaggle/input/article-classification/train.csv')\n",
    "        df_test_orig = pd.read_csv('/kaggle/input/article-classification/test.csv')\n",
    "        print(\"Loaded data from Kaggle path.\")\n",
    "        print(f\"Loaded train.csv: {df_train_orig.shape}\")\n",
    "        print(f\"Loaded test.csv: {df_test_orig.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Files also not found at Kaggle path.\")\n",
    "        print(\"Exiting script.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# Handle potential missing values (if any)\n",
    "df_train_orig.dropna(subset=['document', 'category'], inplace=True)\n",
    "df_test_orig.dropna(subset=['document', 'category'], inplace=True)\n",
    "print(\"Handled potential missing values.\")\n",
    "\n",
    "# --- Text Cleaning ---\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = ' '.join([word for word in text.split() if word not in STOP_WORDS and len(word) > 1])\n",
    "    return text\n",
    "\n",
    "print(\"Applying text cleaning to documents...\")\n",
    "df_train_orig['cleaned_document'] = df_train_orig['document'].apply(preprocess_text)\n",
    "df_test_orig['cleaned_document'] = df_test_orig['document'].apply(preprocess_text)\n",
    "print(\"Text cleaning complete.\")\n",
    "\n",
    "# --- Label Parsing and Binarization ---\n",
    "def parse_labels(categories):\n",
    "    if isinstance(categories, str): return [label.strip() for label in categories.split(',')]\n",
    "    return []\n",
    "\n",
    "print(\"\\nParsing and binarizing labels...\")\n",
    "train_labels_list = df_train_orig['category'].apply(parse_labels).tolist()\n",
    "test_labels_list = df_test_orig['category'].apply(parse_labels).tolist()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_bin = mlb.fit_transform(train_labels_list)\n",
    "known_classes = set(mlb.classes_)\n",
    "test_labels_list_filtered = [[lbl for lbl in labels if lbl in known_classes] for labels in test_labels_list]\n",
    "y_test_bin = mlb.transform(test_labels_list_filtered)\n",
    "\n",
    "print(f\"Number of unique labels found: {len(mlb.classes_)}\")\n",
    "print(f\"Shape of binarized training labels: {y_train_bin.shape}\") # (samples, labels)\n",
    "print(f\"Shape of binarized test labels: {y_test_bin.shape}\")   # (samples, labels)\n",
    "print(\"Labels binarized.\")\n",
    "\n",
    "# --- TF-IDF from Scratch ---\n",
    "# (TF-IDF Class definition remains the same as before)\n",
    "class TfidfVectorizerScratch:\n",
    "    def __init__(self, max_features=None):\n",
    "        self.max_features = max_features\n",
    "        self._vocab = {}\n",
    "        self._idf = {}\n",
    "        self._feature_names = []\n",
    "\n",
    "    def _build_vocab(self, corpus):\n",
    "        doc_freq = defaultdict(int)\n",
    "        term_counts = Counter()\n",
    "        total_docs = len(corpus)\n",
    "        for doc in corpus:\n",
    "            seen_in_doc = set()\n",
    "            words = doc.split()\n",
    "            term_counts.update(words)\n",
    "            for word in words:\n",
    "                if word not in seen_in_doc:\n",
    "                    doc_freq[word] += 1\n",
    "                    seen_in_doc.add(word)\n",
    "        temp_idf = {}\n",
    "        for word, freq in doc_freq.items():\n",
    "            temp_idf[word] = math.log(total_docs / (freq + 1)) + 1\n",
    "        if self.max_features and len(temp_idf) > self.max_features:\n",
    "            sorted_terms = sorted(temp_idf.items(), key=lambda item: (item[1], term_counts[item[0]]), reverse=True)\n",
    "            top_terms = [term for term, score in sorted_terms[:self.max_features]]\n",
    "            self._vocab = {term: i for i, term in enumerate(top_terms)}\n",
    "            self._feature_names = top_terms\n",
    "            self._idf = {term: temp_idf[term] for term in top_terms}\n",
    "            print(f\"Vocabulary size limited to top {self.max_features} features by IDF.\")\n",
    "        else:\n",
    "            self._vocab = {word: i for i, word in enumerate(doc_freq.keys())}\n",
    "            self._idf = temp_idf\n",
    "            self._feature_names = list(self._vocab.keys())\n",
    "            print(f\"Vocabulary size: {len(self._vocab)}\")\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self._build_vocab(corpus)\n",
    "        return self\n",
    "    def transform(self, corpus):\n",
    "        n_docs = len(corpus)\n",
    "        n_features = len(self._vocab)\n",
    "        tfidf_matrix = np.zeros((n_docs, n_features))\n",
    "        for i, doc in enumerate(corpus):\n",
    "            words = doc.split()\n",
    "            doc_len = len(words)\n",
    "            if doc_len == 0: continue\n",
    "            term_counts_in_doc = Counter(words)\n",
    "            for word, count in term_counts_in_doc.items():\n",
    "                if word in self._vocab:\n",
    "                    tf = count / doc_len\n",
    "                    idf = self._idf.get(word, 0)\n",
    "                    feature_index = self._vocab[word]\n",
    "                    tfidf_matrix[i, feature_index] = tf * idf\n",
    "        return tfidf_matrix\n",
    "    def fit_transform(self, corpus):\n",
    "        self.fit(corpus)\n",
    "        return self.transform(corpus)\n",
    "    def get_feature_names_out(self):\n",
    "        return self._feature_names\n",
    "\n",
    "print(\"\\nCalculating TF-IDF features from scratch...\")\n",
    "vectorizer = TfidfVectorizerScratch(max_features=MAX_FEATURES)\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train_orig['cleaned_document'])\n",
    "X_test_tfidf = vectorizer.transform(df_test_orig['cleaned_document'])\n",
    "print(f\"Shape of TF-IDF training features: {X_train_tfidf.shape}\") # (samples, features)\n",
    "print(f\"Shape of TF-IDF test features: {X_test_tfidf.shape}\")   # (samples, features)\n",
    "print(\"TF-IDF calculation complete.\")\n",
    "\n",
    "# --- Splitting Training Data ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_tfidf, y_train_bin, test_size=0.15, random_state=42\n",
    ")\n",
    "print(\"\\nSplit training data into training and validation sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\") # (samples, features/labels)\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")       # (samples, features/labels)\n",
    "print(f\"X_test shape: {X_test_tfidf.shape}, y_test shape: {y_test_bin.shape}\") # (samples, features/labels)\n",
    "\n",
    "# --- Activation Functions (Same as before) ---\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "    return A, Z\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, Z\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    return A, Z\n",
    "def tanh_backward(dA, cache):\n",
    "    Z = cache\n",
    "    t = np.tanh(Z)\n",
    "    dZ = dA * (1 - np.power(t, 2))\n",
    "    return dZ\n",
    "\n",
    "# --- MLP Class with Momentum ---\n",
    "class MLPClassifierMultiLabel:\n",
    "    def __init__(self, layer_dims, learning_rate=0.01, activation='relu',\n",
    "                 optimizer='minibatch', momentum=0.9, # Added momentum param\n",
    "                 epochs=100, batch_size=64,\n",
    "                 random_state=None, print_cost_every=10):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_activation_name = activation.lower()\n",
    "        self.optimizer = optimizer.lower() # Keep optimizer param for potential future extension (sgd, batch)\n",
    "        self.momentum = momentum # Store momentum factor\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.print_cost_every = print_cost_every\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.v = {} # Initialize velocity dictionary for momentum\n",
    "        self.costs_train = []\n",
    "        self.costs_val = []\n",
    "        self.hidden_activation_forward = None\n",
    "        self.hidden_activation_backward = None\n",
    "        self.output_activation_forward = sigmoid\n",
    "        self.output_activation_backward = sigmoid_backward\n",
    "\n",
    "        self._select_hidden_activation()\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _select_hidden_activation(self):\n",
    "        if self.hidden_activation_name == 'sigmoid': self.hidden_activation_forward, self.hidden_activation_backward = sigmoid, sigmoid_backward\n",
    "        elif self.hidden_activation_name == 'tanh': self.hidden_activation_forward, self.hidden_activation_backward = tanh, tanh_backward\n",
    "        elif self.hidden_activation_name == 'relu': self.hidden_activation_forward, self.hidden_activation_backward = relu, relu_backward\n",
    "        else: raise ValueError(\"Invalid hidden activation. Choose 'sigmoid', 'tanh', or 'relu'.\")\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        if self.random_state is not None: np.random.seed(self.random_state)\n",
    "        L = len(self.layer_dims)\n",
    "        for l in range(1, L):\n",
    "            scale_factor = np.sqrt(1. / self.layer_dims[l-1])\n",
    "            if self.hidden_activation_name == 'relu' and l < L - 1: scale_factor = np.sqrt(2. / self.layer_dims[l-1])\n",
    "            elif self.hidden_activation_name == 'relu' and l == L - 1: scale_factor = np.sqrt(1. / self.layer_dims[l-1])\n",
    "\n",
    "            self.parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1]) * scale_factor\n",
    "            self.parameters['b' + str(l)] = np.zeros((self.layer_dims[l], 1))\n",
    "\n",
    "            # Initialize velocity terms for momentum with zeros\n",
    "            self.v[\"dW\" + str(l)] = np.zeros_like(self.parameters[\"W\" + str(l)])\n",
    "            self.v[\"db\" + str(l)] = np.zeros_like(self.parameters[\"b\" + str(l)])\n",
    "\n",
    "    # --- Forward Propagation (Same as before) ---\n",
    "    def _linear_forward(self, A_prev, W, b):\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        return Z, (A_prev, W, b)\n",
    "    def _linear_activation_forward(self, A_prev, W, b, activation_func):\n",
    "        Z, linear_cache = self._linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = activation_func(Z)\n",
    "        return A, (linear_cache, activation_cache)\n",
    "    def _forward_propagation(self, X):\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(1, L):\n",
    "            A, cache = self._linear_activation_forward(A, self.parameters['W' + str(l)], self.parameters['b' + str(l)], self.hidden_activation_forward)\n",
    "            caches.append(cache)\n",
    "        AL, cache_L = self._linear_activation_forward(A, self.parameters['W' + str(L)], self.parameters['b' + str(L)], self.output_activation_forward)\n",
    "        caches.append(cache_L)\n",
    "        if np.any(np.isnan(AL)) or np.any(np.isinf(AL)): print(\"Warning: NaN/Inf in AL\")\n",
    "        return AL, caches\n",
    "\n",
    "    # --- Cost Function (Same as before) ---\n",
    "    def _compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        epsilon = 1e-8\n",
    "        AL_clipped = np.clip(AL, epsilon, 1 - epsilon)\n",
    "        cost = -(1/m) * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n",
    "        cost = np.squeeze(cost)\n",
    "        if np.isnan(cost) or np.isinf(cost):\n",
    "             print(\"Warning: NaN or Inf detected in cost calculation.\")\n",
    "             return np.inf\n",
    "        return cost\n",
    "\n",
    "    # --- Backward Propagation (Same as before) ---\n",
    "    def _linear_backward(self, dZ, linear_cache):\n",
    "        A_prev, W, b = linear_cache\n",
    "        m = A_prev.shape[1]\n",
    "        dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        return dA_prev, dW, db\n",
    "    def _linear_activation_backward(self, dA, cache, activation_backward_func):\n",
    "        linear_cache, activation_cache = cache\n",
    "        dZ = activation_backward_func(dA, activation_cache)\n",
    "        if np.any(np.isnan(dZ)) or np.any(np.isinf(dZ)): print(f\"Warning: NaN/Inf in dZ\")\n",
    "        dA_prev, dW, db = self._linear_backward(dZ, linear_cache)\n",
    "        return dA_prev, dW, db\n",
    "    def _backward_propagation(self, AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        epsilon = 1e-8\n",
    "        AL_clipped = np.clip(AL, epsilon, 1 - epsilon)\n",
    "        dAL = - (np.divide(Y, AL_clipped) - np.divide(1 - Y, 1 - AL_clipped))\n",
    "        if np.any(np.isnan(dAL)) or np.any(np.isinf(dAL)):\n",
    "            print(\"Warning: NaN or Inf detected in dAL. Replacing.\")\n",
    "            dAL = np.nan_to_num(dAL, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_L, dW_L, db_L = self._linear_activation_backward(dAL, current_cache, self.output_activation_backward)\n",
    "        grads[\"dA\" + str(L-1)] = dA_prev_L\n",
    "        grads[\"dW\" + str(L)] = dW_L\n",
    "        grads[\"db\" + str(L)] = db_L\n",
    "        for l in reversed(range(L - 1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_curr = grads[\"dA\" + str(l + 1)]\n",
    "            dA_prev, dW_curr, db_curr = self._linear_activation_backward(dA_curr, current_cache, self.hidden_activation_backward)\n",
    "            grads[\"dA\" + str(l)] = dA_prev\n",
    "            grads[\"dW\" + str(l + 1)] = dW_curr\n",
    "            grads[\"db\" + str(l + 1)] = db_curr\n",
    "        for key, value in grads.items():\n",
    "            if np.any(np.isnan(value)) or np.any(np.isinf(value)):\n",
    "                print(f\"Warning: NaN or Inf detected in final gradient '{key}'. Replacing.\")\n",
    "                grads[key] = np.nan_to_num(value, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        return grads\n",
    "\n",
    "    # --- Update Parameters (Minibatch GD with Momentum) ---\n",
    "    def _update_parameters(self, grads):\n",
    "        \"\"\"Updates parameters using Minibatch Gradient Descent with Momentum.\"\"\"\n",
    "        L = len(self.parameters) // 2\n",
    "\n",
    "        for l in range(1, L + 1):\n",
    "            # Retrieve gradients\n",
    "            dW = grads[\"dW\" + str(l)]\n",
    "            db = grads[\"db\" + str(l)]\n",
    "\n",
    "            # --- Momentum Update ---\n",
    "            # Update velocity\n",
    "            self.v[\"dW\" + str(l)] = self.momentum * self.v[\"dW\" + str(l)] + (1 - self.momentum) * dW\n",
    "            self.v[\"db\" + str(l)] = self.momentum * self.v[\"db\" + str(l)] + (1 - self.momentum) * db\n",
    "\n",
    "            # Update parameters using velocity\n",
    "            self.parameters[\"W\" + str(l)] -= self.learning_rate * self.v[\"dW\" + str(l)]\n",
    "            self.parameters[\"b\" + str(l)] -= self.learning_rate * self.v[\"db\" + str(l)]\n",
    "            # --- End Momentum Update ---\n",
    "\n",
    "    # --- Mini-batch Creation (Same as before) ---\n",
    "    def _create_mini_batches(self, X, Y, batch_size, seed):\n",
    "        X_contig = np.asfortranarray(X)\n",
    "        Y_contig = np.asfortranarray(Y)\n",
    "        np.random.seed(seed)\n",
    "        m = X.shape[1]\n",
    "        mini_batches = []\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X_contig[:, permutation]\n",
    "        shuffled_Y = Y_contig[:, permutation]\n",
    "        num_complete_minibatches = math.floor(m / batch_size)\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * batch_size : (k + 1) * batch_size]\n",
    "            mini_batch_Y = shuffled_Y[:, k * batch_size : (k + 1) * batch_size]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size :]\n",
    "            mini_batch_Y = shuffled_Y[:, num_complete_minibatches * batch_size :]\n",
    "            mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "        return mini_batches\n",
    "\n",
    "    # --- Training Loop (Uses Momentum Update) ---\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        if self.random_state is not None: np.random.seed(self.random_state)\n",
    "        seed = self.random_state if self.random_state is not None else 0\n",
    "        m_train = X_train.shape[1]\n",
    "        self.costs_train = []\n",
    "        self.costs_val = []\n",
    "\n",
    "        print(f\"Starting training: {self.epochs} epochs, LR={self.learning_rate}, BatchSize={self.batch_size}, Optimizer={self.optimizer} w/ Momentum={self.momentum}\")\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            epoch_cost_train = 0.\n",
    "            seed += 1\n",
    "\n",
    "            if self.optimizer == 'minibatch':\n",
    "                mini_batches = self._create_mini_batches(X_train, y_train, self.batch_size, seed)\n",
    "                current_epoch_costs = []\n",
    "                num_batches = len(mini_batches)\n",
    "                for batch_idx, mini_batch in enumerate(mini_batches):\n",
    "                    (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "                    AL, caches = self._forward_propagation(mini_batch_X)\n",
    "                    cost_mini_batch = self._compute_cost(AL, mini_batch_Y)\n",
    "                    if np.isnan(cost_mini_batch) or np.isinf(cost_mini_batch):\n",
    "                         print(f\"Warning: NaN/Inf cost encountered in epoch {i}, batch {batch_idx+1}/{num_batches}. Skipping grad update.\")\n",
    "                         continue\n",
    "                    current_epoch_costs.append(cost_mini_batch)\n",
    "                    grads = self._backward_propagation(AL, mini_batch_Y, caches)\n",
    "                    self._update_parameters(grads) # Updates with momentum\n",
    "                if current_epoch_costs: epoch_cost_train = np.mean(current_epoch_costs)\n",
    "                else: epoch_cost_train = np.nan\n",
    "            # Add 'sgd' or 'batch' options here if needed, updating parameters accordingly\n",
    "            else:\n",
    "                 raise ValueError(\"Invalid optimizer. Only 'minibatch' supported in this version.\")\n",
    "\n",
    "            # --- Validation cost ---\n",
    "            AL_val, _ = self._forward_propagation(X_val)\n",
    "            cost_val = self._compute_cost(AL_val, y_val)\n",
    "\n",
    "            # Store costs\n",
    "            if not (np.isnan(epoch_cost_train) or np.isinf(epoch_cost_train)): self.costs_train.append(epoch_cost_train)\n",
    "            else: self.costs_train.append(self.costs_train[-1] if self.costs_train else np.nan)\n",
    "            if not (np.isnan(cost_val) or np.isinf(cost_val)): self.costs_val.append(cost_val)\n",
    "            else: self.costs_val.append(self.costs_val[-1] if self.costs_val else np.nan)\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "            if self.print_cost_every > 0 and i % self.print_cost_every == 0:\n",
    "                print(f\"Epoch {i}/{self.epochs-1} | Train Cost: {epoch_cost_train:.6f} | Val Cost: {cost_val:.6f} | Time: {epoch_duration:.2f}s\")\n",
    "            elif i == self.epochs - 1:\n",
    "                print(f\"Epoch {i}/{self.epochs-1} | Train Cost: {epoch_cost_train:.6f} | Val Cost: {cost_val:.6f} | Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Prediction (Same as before) ---\n",
    "    def predict_proba(self, X):\n",
    "        AL, _ = self._forward_propagation(X)\n",
    "        return AL\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas > threshold).astype(int)\n",
    "\n",
    "    # --- Evaluation (Same as before, with corrected shape check) ---\n",
    "    def evaluate(self, X, y_true_bin, threshold=0.5):\n",
    "        y_pred_prob_mlp = self.predict_proba(X)\n",
    "        y_pred_bin_mlp = (y_pred_prob_mlp > threshold).astype(int)\n",
    "        y_pred_bin_sklearn = y_pred_bin_mlp.T\n",
    "\n",
    "        n_samples_eval = X.shape[1]\n",
    "        n_labels_eval = self.layer_dims[-1]\n",
    "        if y_true_bin.shape == (n_samples_eval, n_labels_eval):\n",
    "            y_true_bin_sklearn = y_true_bin\n",
    "        elif y_true_bin.shape == (n_labels_eval, n_samples_eval):\n",
    "            y_true_bin_sklearn = y_true_bin.T\n",
    "            print(f\"Transposing y_true_bin from {y_true_bin.shape} to {y_true_bin_sklearn.shape} for evaluation.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected shape for y_true_bin: {y_true_bin.shape}. Expected ({n_samples_eval}, {n_labels_eval}) or ({n_labels_eval}, {n_samples_eval})\")\n",
    "\n",
    "        if y_true_bin_sklearn.shape != y_pred_bin_sklearn.shape:\n",
    "             raise ValueError(f\"Final shape mismatch: y_true_bin_sklearn {y_true_bin_sklearn.shape} vs y_pred_bin_sklearn {y_pred_bin_sklearn.shape}\")\n",
    "\n",
    "        acc = accuracy_score(y_true_bin_sklearn, y_pred_bin_sklearn)\n",
    "        hamming = hamming_loss(y_true_bin_sklearn, y_pred_bin_sklearn)\n",
    "        f1_micro = f1_score(y_true_bin_sklearn, y_pred_bin_sklearn, average='micro', zero_division=0)\n",
    "        f1_macro = f1_score(y_true_bin_sklearn, y_pred_bin_sklearn, average='macro', zero_division=0)\n",
    "        f1_samples = f1_score(y_true_bin_sklearn, y_pred_bin_sklearn, average='samples', zero_division=0)\n",
    "        return acc, hamming, f1_micro, f1_macro, f1_samples\n",
    "\n",
    "print(\"\\nMLPClassifierMultiLabel class defined (with Momentum).\")\n",
    "\n",
    "# --- 2.3.3 Hyperparameter Tuning & Evaluation ---\n",
    "print(\"\\n--- Hyperparameter Tuning & Evaluation ---\")\n",
    "\n",
    "# Prepare data shapes (features, examples)\n",
    "X_train_mlp = X_train.T\n",
    "y_train_mlp = y_train.T\n",
    "X_val_mlp = X_val.T\n",
    "y_val_mlp = y_val.T\n",
    "X_test_mlp = X_test_tfidf.T\n",
    "# y_test_bin shape is (samples, labels) - passed directly to evaluate\n",
    "\n",
    "n_features = X_train_mlp.shape[0]\n",
    "n_labels = y_train_mlp.shape[0]\n",
    "print(f\"Input features: {n_features}, Output labels: {n_labels}\")\n",
    "print(f\"Data shapes for MLP: \")\n",
    "print(f\"  X_train_mlp: {X_train_mlp.shape}, y_train_mlp: {y_train_mlp.shape}\")\n",
    "print(f\"  X_val_mlp:   {X_val_mlp.shape}, y_val_mlp:   {y_val_mlp.shape}\")\n",
    "print(f\"  X_test_mlp:  {X_test_mlp.shape}, y_test_bin:  {y_test_bin.shape}\")\n",
    "\n",
    "# --- Adjusted Hyperparameter Space (for GD + Momentum) ---\n",
    "learning_rates = [0.01, 0.005, 0.001] # Smaller LRs more suitable\n",
    "epochs_list = [150, 250]              # More epochs needed potentially\n",
    "architectures = [\n",
    "    [n_features, 128, n_labels],\n",
    "    [n_features, 256, 128, n_labels]\n",
    "]\n",
    "hidden_activations = ['relu', 'tanh']\n",
    "optimizers = ['minibatch'] # Conceptually minibatch GD (+momentum internally)\n",
    "batch_sizes = [32, 64]\n",
    "# Momentum fixed at 0.9 in __init__, but could be added to tune here\n",
    "\n",
    "results = []\n",
    "iteration = 0\n",
    "total_iterations = len(learning_rates) * len(epochs_list) * len(architectures) * len(hidden_activations) * len(optimizers) * len(batch_sizes)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        for arch in architectures:\n",
    "            for activation in hidden_activations:\n",
    "                for optimizer in optimizers: # Loop still here, but only one option\n",
    "                    for batch_size in batch_sizes:\n",
    "                        iteration += 1\n",
    "                        print(f\"\\n--- Running Iteration {iteration}/{total_iterations} ---\")\n",
    "                        print(f\"Config: LR={lr}, Epochs={epochs}, Arch={arch}, Act={activation}, Opt={optimizer}+Momentum, Batch={batch_size}\")\n",
    "\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        mlp = MLPClassifierMultiLabel(layer_dims=arch,\n",
    "                                           learning_rate=lr,\n",
    "                                           activation=activation,\n",
    "                                           optimizer=optimizer,\n",
    "                                           momentum=0.9, # Using fixed momentum\n",
    "                                           epochs=epochs,\n",
    "                                           batch_size=batch_size,\n",
    "                                           random_state=42,\n",
    "                                           print_cost_every=20) # Print less often for longer epochs\n",
    "\n",
    "                        # Train the model\n",
    "                        mlp.fit(X_train_mlp, y_train_mlp, X_val_mlp, y_val_mlp)\n",
    "\n",
    "                        end_time = time.time()\n",
    "                        training_time = end_time - start_time\n",
    "\n",
    "                        # Evaluate on the Test set\n",
    "                        acc, hamming, f1_micro, f1_macro, f1_samples = mlp.evaluate(X_test_mlp, y_test_bin, threshold=0.5)\n",
    "\n",
    "                        print(f\"Test Set Results:\")\n",
    "                        print(f\"  Accuracy(Subset) = {acc:.4f}\")\n",
    "                        print(f\"  Hamming Loss     = {hamming:.4f}\")\n",
    "                        print(f\"  F1 Micro         = {f1_micro:.4f}\")\n",
    "                        print(f\"  F1 Macro         = {f1_macro:.4f}\")\n",
    "                        print(f\"  F1 Samples       = {f1_samples:.4f}\")\n",
    "                        print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "                        # Store results\n",
    "                        results.append({\n",
    "                            'learning_rate': lr,\n",
    "                            'epochs': epochs,\n",
    "                            'architecture': str(arch),\n",
    "                            'activation': activation,\n",
    "                            'optimizer': optimizer + '+Momentum', # Indicate momentum used\n",
    "                            'batch_size': batch_size,\n",
    "                            'accuracy': acc,\n",
    "                            'hamming_loss': hamming,\n",
    "                            'f1_micro': f1_micro,\n",
    "                            'f1_macro': f1_macro,\n",
    "                            'f1_samples': f1_samples,\n",
    "                            'training_time': training_time,\n",
    "                            'train_costs': mlp.costs_train,\n",
    "                            'val_costs': mlp.costs_val\n",
    "                        })\n",
    "\n",
    "                        # Optional prediction inspection\n",
    "                        # print(\"\\n--- Prediction Inspection ---\")\n",
    "                        # y_pred_probs = mlp.predict_proba(X_test_mlp)\n",
    "                        # print(f\"Max predicted probability across all labels for first 5 samples: {np.max(y_pred_probs[:, :5], axis=0)}\")\n",
    "                        # print(f\"Mean predicted probability across all labels/samples: {np.mean(y_pred_probs)}\")\n",
    "                        # y_pred_bin = mlp.predict(X_test_mlp, threshold=0.5)\n",
    "                        # y_pred_bin_sum = np.sum(y_pred_bin, axis=0)\n",
    "                        # print(f\"Number of predicted labels for first 5 samples: {y_pred_bin_sum[:5]}\")\n",
    "                        # print(f\"Total predicted labels across test set: {np.sum(y_pred_bin_sum)}\")\n",
    "                        # print(\"--- End Inspection ---\")\n",
    "\n",
    "\n",
    "# --- Reporting (Same as before) ---\n",
    "print(\"\\n\\n--- Experiment Results ---\")\n",
    "if not results: print(\"No results generated.\")\n",
    "else:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df_sorted = results_df.sort_values(by='f1_micro', ascending=False).reset_index(drop=True)\n",
    "    print(\"\\nTop 5 Configurations (Sorted by F1 Micro Score):\")\n",
    "    print(results_df_sorted[['learning_rate', 'epochs', 'activation', 'optimizer', 'batch_size', 'architecture', 'f1_micro', 'accuracy', 'hamming_loss', 'f1_samples']].head())\n",
    "    print(\"\\nFull Results Table (Sorted by F1 Micro Score):\")\n",
    "    print(results_df_sorted[['learning_rate', 'epochs', 'activation', 'optimizer', 'batch_size', 'architecture', 'f1_micro', 'accuracy', 'hamming_loss', 'f1_macro','f1_samples', 'training_time']])\n",
    "\n",
    "    best_config_index = results_df_sorted.index[0]\n",
    "    original_index = results_df[\n",
    "        (results_df['learning_rate'] == results_df_sorted.loc[best_config_index, 'learning_rate']) &\n",
    "        (results_df['epochs'] == results_df_sorted.loc[best_config_index, 'epochs']) &\n",
    "        (results_df['architecture'] == results_df_sorted.loc[best_config_index, 'architecture']) &\n",
    "        (results_df['activation'] == results_df_sorted.loc[best_config_index, 'activation']) &\n",
    "        (results_df['optimizer'] == results_df_sorted.loc[best_config_index, 'optimizer']) & # Match optimizer name\n",
    "        (results_df['batch_size'] == results_df_sorted.loc[best_config_index, 'batch_size'])\n",
    "    ].index[0]\n",
    "    best_config_dict = results[original_index]\n",
    "    best_config = results_df_sorted.iloc[best_config_index]\n",
    "\n",
    "    print(f\"\\nBest Configuration Found (based on F1 Micro):\")\n",
    "    print(f\"  Learning Rate: {best_config['learning_rate']}\")\n",
    "    print(f\"  Epochs: {best_config['epochs']}\")\n",
    "    print(f\"  Architecture: {best_config['architecture']}\")\n",
    "    print(f\"  Hidden Activation: {best_config['activation']}\")\n",
    "    print(f\"  Optimizer: {best_config['optimizer']}\") # Will show 'minibatch+Momentum'\n",
    "    print(f\"  Batch Size: {best_config['batch_size']}\")\n",
    "    print(f\"  Test F1 Micro: {best_config['f1_micro']:.4f}\")\n",
    "    print(f\"  Test Accuracy (Subset): {best_config['accuracy']:.4f}\")\n",
    "    print(f\"  Test Hamming Loss: {best_config['hamming_loss']:.4f}\")\n",
    "    print(f\"  Test F1 Macro: {best_config['f1_macro']:.4f}\")\n",
    "    print(f\"  Test F1 Samples: {best_config['f1_samples']:.4f}\")\n",
    "\n",
    "    # --- Plotting (Same as before) ---\n",
    "    print(\"\\n--- Plotting Training Curves for Best Configuration ---\")\n",
    "    best_train_costs = best_config_dict['train_costs']\n",
    "    best_val_costs = best_config_dict['val_costs']\n",
    "    epochs_axis_train = [i for i, cost in enumerate(best_train_costs) if not (np.isnan(cost) or np.isinf(cost))]\n",
    "    valid_train_costs = [cost for cost in best_train_costs if not (np.isnan(cost) or np.isinf(cost))]\n",
    "    epochs_axis_val = [i for i, cost in enumerate(best_val_costs) if not (np.isnan(cost) or np.isinf(cost))]\n",
    "    valid_val_costs = [cost for cost in best_val_costs if not (np.isnan(cost) or np.isinf(cost))]\n",
    "    if not valid_train_costs or not valid_val_costs: print(\"Warning: No valid costs to plot.\")\n",
    "    else:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epochs_axis_train, valid_train_costs, label='Training Loss (BCE)', marker='.', linestyle='-')\n",
    "        plt.plot(epochs_axis_val, valid_val_costs, label='Validation Loss (BCE)', marker='.', linestyle='-')\n",
    "        plt.title(f'Training and Validation Loss (Best Config: {best_config[\"activation\"]}/{best_config[\"optimizer\"]}, LR={best_config[\"learning_rate\"]}, Batch={best_config[\"batch_size\"]})')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost (Binary Cross-Entropy)')\n",
    "        plt.legend(); plt.grid(True)\n",
    "        min_cost = min(min(valid_train_costs), min(valid_val_costs)); max_cost = max(max(valid_train_costs), max(valid_val_costs))\n",
    "        cost_range = max_cost - min_cost; padding = max(0.1 * cost_range, 0.1)\n",
    "        plt.ylim(max(0, min_cost - padding), max_cost + padding); plt.show()\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7162032,
     "sourceId": 11434552,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36300.02508,
   "end_time": "2025-04-16T23:28:16.063683",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-16T13:23:16.038603",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
